[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "MATH 240 - Basic Statistics with R",
    "section": "",
    "text": "Instructor: Dr. Joash Geteregechi\nContact: jgeteregechi@ithaca.edu.\nOffice Location: Williams Hall 311E. If you use the elevator, press 3 (not 3R). If you use the staircase, enter the door written “Faculty Offices” on the third floor.\nOpen Hours: Mon 1:00-2:00 pm, Fri 10.00 - 11:00 am, or by appointment: click here.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#about-the-course",
    "href": "course-syllabus.html#about-the-course",
    "title": "MATH 240 - Basic Statistics with R",
    "section": "About the Course",
    "text": "About the Course\n\nCourse Description\nAn introduction to R, a free scientific computing platform, as applied to basic statistics. Students learn how to manage data, create appropriate graphs, perform basic statistical tests (t-tests, chi-square, ANOVA, regression, etc), and compute confidence intervals. Additional topics include tests of assumptions (e.g., normality) and post hoc tests.\n\n\nPrerequisites and Credits\nPrerequisites include MATH 14400, MATH 14500, MATH 21600, MATH 24600 or PSYC 20700 with a grade of C- or better. (S,Y).\nWorkload Expectation:\nThis is a 1-credit course. Credit is earned at Ithaca College in credit hours as measured by the Carnegie unit. The Carnegie unit is defined as one hour of classroom instruction and two hours of assignments outside the classroom, for a period of 15 weeks for each unit (credit).\n\n\nMeeting Times\n\n\n\nDay\nRoom/Hall\nTime\n\n\n\n\nMonday\nWilliams Hall 310\n10:00 - 11:00 am\n\n\n\n\n\nStudent Learning Objectives\nThe course has two main learning objectives:\n\nDemonstrate proficiency with basic procedures in R, including importing data, preparing data for analysis(data wrangling), summarizing data (numerically and graphically), and performing descriptive and inferential statistics.\nApply statistical tools in the real-world (especially answering a research question using real-world data).",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-resources",
    "href": "course-syllabus.html#course-resources",
    "title": "MATH 240 - Basic Statistics with R",
    "section": "Course Resources",
    "text": "Course Resources\n\nTextbooks\nThere is no textbook to purchase. All materials will be provided by the instructor.\n\n\nComputing:\n\nR: We will use R statistical environment with the RStudio interface. You’ll be using R primarily through a version of RStudio accessible on posit cloud. Once you are on this page, sign in or click on sign up to create a new account in case you don’t already have an account.\nGitHub: GitHub is a web-based platform that allows people to store, share, and manage versions of their code. For example, a team of programmers working on different parts of a certain project may use GitHub for collaboration. We shall use these features only sparingly in this course. Copilot, on the other hand, is a popular AI-powered tool used for a vast range of purposes including code generation. Copilot is a paid service but college students can access and use it for free via GitHub (GitHub Copilot). To use GitHub copilot and use it within RStudio, there are three steps involved:\nStep 1: Sign up for a free GitHub account at GitHub.\nStep 2: Sign up for a free GitHub Student Developer Pack account at GitHub Studente Developer.\nStep 3: Install the GitHub Copilot extension in RStudio to activate your Copilot in RStudio.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "MATH 240 - Basic Statistics with R",
    "section": "Assessment",
    "text": "Assessment\nYour learning will be assessed primarily via weekly labs and a final project.\n\nAttendance and Participation (AP)\nYou’re expected to attend all classes and participate during class discussions. Consistent attendance and participation are strong indicators of success in MATH classes. You will be responsible for all course material, announcements, assessments done in class, whether you attended that day’s class or not. I may post the announcements and materials on Canvas and/or send them out as emails. You are responsible for checking your emails or Canvas website regularly (at least one a day).\nIf you anticipate needing to miss more than two class sessions, be sure to discuss this with me early in the semester so we can find ways for you to participate and be successful in the course.\nPlease check the college attendance policy on the college-wide policies section.\n\n\nWeekly Labs\nThere will be a Lab assignment every week. Initially, the assignments will be very similar to the lab explorations (done in class) but as the semester progresses, you will encounter more challenging tasks. Labs will be available on the course website every week.\n\n\nFinal Project\nThere will be one final project in this course. The project will be done in groups of two to 3 students. More details will be available later.\n\n\nGrading Policy\nYour final letter grade in the course will be weighted by category as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nAttendance and Participation\n10%\n\n\nWeekly Labs\n40%\n\n\nFinal Project\n50%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n\\(\\ge\\) 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n\\(&lt;\\) 60",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "MATH 240 - Basic Statistics with R",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nThe point is very simple - you should not cheat. You should not present “someone” else’s work as your own.\nAbide by the following guidelines:\n\nCollaboration:\n\nWork that is not assigned as a collaborative assignment should not be completed collaboratively. This does not mean you should not seek support from peers. If you seek help from your peers, be sure that you write your own solutions, otherwise the work will be similar and flagged for cheating. Submitting similar work will be considered a violation of the academic integrity policy by all students involved.\nFor team assignments, you may collaborate freely within your team. Each group will submit one document contained agreed upon responses. No multiple file submissions.\n\nOn individual assignments you may not directly share work with another student in this class, and on team assignments you may not directly share work with another team in this class.\n\nOnline resources: In this century, the internet is a go to place for many things. While much of the information on the internet is useful, I expect that you will use it responsibly. The course policy is that you may use online resources (e.g., StackOverflow, Wolfram Alpha, etc.) but you must explicitly cite where you obtained any solutions you directly use (or use as inspiration).\nUse of generative artificial intelligence (AI): Generative AI tools such as ChatGPT should be treated as other online resources. There are two guiding principles that govern how you can use AI in this course:\n\nCognitive dimension: Using AI tools should make you more efficient and productive rather than hampering your ability to think clearly and critically.\nEthical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content. AI is an integral part of this course and we will practice using generative AI tools responsibly without losing sight of the learning objectives.\n✅ AI tools: If you use AI in any way for course assignments, be sure to disclose it in your submission. Follow these guidelines for citing AI-generated content.\n❌ AI warning AI-generated content may be inaccurate and even misleading in some cases. You will likely be temoted to use AI to write essays for this class but be warned that doing so in akin to copying and pasting someone else’s work. I suggest you use AI only to check your grammar or to generate an initial document that you should then sit down and edit to make sure it meets the need. Your experiences in this course will be unique and AI does not have any knowledge about these so it cannot generate a great essay.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please contact your instructor.\n\n\nLate Work\nThe due date & time for all assignments will be posted on Canvas and/or emailed out and/or announced in class. To enable me to prepare for class meetings and give you feedback, I will not accept late work except under extreme circumstances. If you know that you won’t be able to turn in an assignment on time, reach out to me in writing (email) at least one day before the due date to discuss your options.\n\n\nMobile devices & Other Technologies\nThis class allows use of technology devices (e.g., computers, tablets, etc.) only for purposes of the course. Such purpose includes taking notes, completing OneNote collaborative activities, among others. However, there will be moments (e.g., brief interactive discussion, completing paper activities) when I require that students put away (or turn off) their technology devices. Use of these devices for purposes other than the one for the course is prohibited. Research on this matter shows that it distracts you as well as other students in class. Two violations per week will result in a 0 score on the next attendance and participation grade. Persistent violation may necessitate further action to prevent you from distracting other students.\n\n\nTeams\nThis class will mostly run through small group work (teams). You will be randomly assigned to a team at the start of the semester. About midway into the semester, I will switch people teams based partly on my professional judgement. I will allow you to suggest members you would like to work with but there are no guarantees that you will get grouped with all members of your choice. I generally seek to have mixed ability teams and to ensure that everyone has a chance to work with different people. It is expected that every team member will contribute equally on team assignments.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#getting-support",
    "href": "course-syllabus.html#getting-support",
    "title": "MATH 240 - Basic Statistics with R",
    "section": "Getting Support",
    "text": "Getting Support\nThere are various resources available to help you succeed in this course. Should you feel like you are struggling too much, please don’t hesitate to reach out to me so we can discuss possible ways forward. Below are some academic support services available to you.\n\nOpen Hours\nI will be available during open hours (Mon 1.00 - 2.00 pm; Fri 10.00-11.00 am) to answer questions or concerns that you may have in the course. You can simply walk in during the stated time above. More times may be available but you will need to check my schedule on this link. Open hours may be held in-person or virtually depending on circumstances of the day. Below are the zoom link and password for virtual meetings:\nZoom Link: click here\nPassword: 850 424\n\n\nMath Tutoring Sessions\nThe mathematics department is committed to the success of all students enrolled in mathematics courses. Free one-on-one support for your mathematics coursework is available during select daytime and evening hours Monday-Friday at the Mathematics Room (Williams Hall 209). The Mathematics Room is staffed by mathematics faculty and vetted students. Student tutors offer support to fellow students in courses numbered 200 and below while math faculty offer support in any of the math courses. For more information and the schedule, please visit the Math Support Center.\n\n\nTutoring and Academic Enrichment Services\nAs a supplement to faculty advising and office hours, Tutoring and Academic Enrichment Services offers exceptional peer resources free of charge. Learning Coaches provide content-specific peer tutoring in a variety of courses. Peer Success Coaches mentor students who wish to develop collegiate-level academic and social engagement skills. To access these courses and for more information, please visit the Center for Student Success.\n\n\nWriting Center\nThe Writing Center aims to help students from all disciplines, backgrounds, and experiences to develop greater independence as writers. We are committed to helping students see writing as central to critical and creative thinking. The physical location in Smiddy 107 will not be open to clients. For more information and scheduling appointments please visit the writing center website.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#tips-for-success",
    "href": "course-syllabus.html#tips-for-success",
    "title": "MATH 240 - Basic Statistics with R",
    "section": "Tips for Success",
    "text": "Tips for Success\nHere are a few basic suggestions for how to succeed in this course:\n\nKeep up with Homework\nIt is absolutely essential that you understand how to solve the assigned homework problems/exercises and, more importantly, how and why the skills and techniques presented in the course are used in solving the problems/exercises. I suggest that you begin working on the how as soon as possible. Do not pile your haw or work near the deadline. The advantage of getting the homework done on time is that you will get timely feedback that will help you understand the material better and hence do well in other assessment categories such as exams.\n\n\nAttend Class\nAs noted earlier, attendance is a critical part of your success in this course. You should try to attend every class because it is during class time that we will delve deeper into the course material and practice with applications.\n\n\nStay Caught Up\nMost concepts in this course build on each other cumulatively and you need to stay on top of the material at every stage. If you are having difficulty, don’t expect that the problem will take care of itself and disappear later. Contact me immediately and discuss the problem.\n\n\nCollaborate with Peers\nMany students benefit from sharing their work with others or by having their work questioned by their peers. You should attempt homework problems ahead of time by yourself and then note down any difficulties/questions that you can discuss with your peers. Even if you have no difficulties, you may still learn different and perhaps more efficient ways of solving the same problem during collaborative work. Below are some of the ways through which you can do this: - Canvas Discussion Forums & One Note Collaboration Space - You can post questions and answer others’ questions here. I encourage you to scan hand-written work (if necessary) and upload it alongside your question so people can see how you are thinking.\n\nZoom sessions – If you cannot meet in person, you can initiate Zoom sessions for collaborating. Zoom whiteboards are available to write on or you may simply have discussions. You can record these for later playback. If you want to invite me to your Zoom session, please send me an email with the link ahead of time and I will let you know if I am available to join.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#college-wide-policies",
    "href": "course-syllabus.html#college-wide-policies",
    "title": "MATH 240 - Basic Statistics with R",
    "section": "College-wide Policies",
    "text": "College-wide Policies\n\nAttendance Policy\nStudents at Ithaca College are expected to attend all classes, and they are responsible for work missed during any absence from class. At the beginning of each semester, instructors must provide the students in their courses with written guidelines regarding possible penalties for failure to attend class. These guidelines may vary from course to course but are subject to the following conditions:\n\nIn accordance with Federal Law, students with a disability documented through Student Accessibility Services (SAS) may require reasonable accommodations to ensure equitable access. A student with an attendance accommodation, who misses a scheduled course time due to a documented disability, must be provided an equivalent opportunity to make up missed time and/or coursework within a reasonable time-frame. An accommodation that affects attendance is not an attendance waiver and no accommodation can fundamentally alter a course requirement. If a faculty member thinks an attendance-related accommodation would result in a fundamental alteration, concerns and potential alternatives should be discussed with SAS.\nIn accordance with New York State law, students who miss class due to their religious beliefs shall be excused from class or examinations on that day. The faculty member is responsible for providing the student with an equivalent opportunity to make up any examination, study, or work requirement that the student may have missed. Any such work is to be completed within a reasonable time frame, as determined by the faculty member.\nAny student who misses class due to a family or individual health emergency or to a required appearance in a court of law shall be excused. If the emergency is prolonged or if the student is incapacitated, the student or a family member/legal guardian should report the absence to the Dean of Students or the Dean of the academic school where the student’s program is housed. Students may consider a leave of absence, medical leave of absence, selected course withdrawals, etc., if they miss a significant portion of classwork. (Note: Graduate students may not take a leave of absence.)\nA student may be excused to participate in local, state, or federal elections. The student is responsible to make up any work that is missed due to the absence. Any such work is to be completed within a reasonable time frame, as determined by the faculty member.\n\nA student may be excused for participation in College-authorized co-curricular and extracurricular activities if, in the instructor’s judgment, this does not impair the specific student’s or the other students’ ability to succeed in the course. For all absences except those due to religious beliefs, the course instructor has the right to determine if the number of absences has been excessive in view of the nature of the class that was missed and the stated attendance policy.\nStudents should notify their instructors as soon as possible of any anticipated absences.\n\n\nStudent Accessibility Services\nIn compliance with Section 504 of the Rehabilitation Act of 1973 and the Americans with Disabilities Act, reasonable accommodation will be provided to students with documented disabilities on a case-by-case basis. Students must register with Student Accessibility Services and provide appropriate documentation to Ithaca College before any academic adjustment will be provided. Please note that accommodations are not retroactive, so timely contact with Student Accessibility Services is encouraged. To discuss accommodations or the accommodation process, students should schedule to meet with a SAS specialist. 607-274-1005 | sas@ithaca.edu.\n\n\nMental Health statement\nThe Ithaca College Center for Counseling and Psychological Services (CAPS) promotes and fosters the academic, personal, and interpersonal development of Ithaca College students by providing short-term individual, group, and relationship counseling, crisis intervention, educational programs to the campus community, and consultation for faculty, staff, parents, and students. Their team of licensed and licensed-eligible professionals value inclusivity, and they are dedicated to creating a diverse, accessible, and welcoming environment that is safe and comfortable for all those they serve and with whom they interact. CAPS sees students in-person at their offices in the Hammond Health building (side entrance), but Telehealth meetings through Zoom can be arranged in some circumstances. Staff in the office will answer questions by phone at 607-274-3136; please leave a voicemail if you do not reach a live person. You can also reach the office via email at counseling@ithaca.edu. CAPS hours remain Monday-Friday 8:30 a.m. to 5:00 p.m. After-hours connections to a live counselor are available by calling the CAPS number and following the prompts.\nIn the event I suspect you need additional support, expect that I will express to you my concerns. It is not my intent to know the details of what might be troubling you, but simply to let you know I am concerned and that help, if needed, is available. Remember, getting help is a smart and courageous thing to do.\n\n\nAcademic Integrity\nThe College is an academic community, which values academic integrity and takes seriously its responsibility for upholding academic honesty. All members of the academic community have an obligation to uphold high intellectual and ethical standards. All forms of dishonesty including cheating and plagiarism are unacceptable. Failure to appropriately cite material used in a paper is plagiarism. The minimum penalty for cheating or plagiarism is a zero for the test or paper in question. Referral to college judiciaries is also possible. For more information on academic integrity and academic dishonesty, please refer to the Student Handbook, the College Catalog and the Code of Student Conduct and Related Policies or ask your instructor.\n\n\nTitle IX\nAt Ithaca College, we believe that every individual has the right to be treated with respect and dignity and we support the creation and maintenance of a safe and positive living and learning environment. Students who experience sexual violence (including dating violence, stalking and sexual assault), sexual harassment, or discrimination based on gender or sexual identity) are encouraged to report their experience to the Title IX Coordinator, lkoenig@ithaca.edu to explore formal and informal reporting options, and explore the support and resources available. The Title IX Coordinator will work with you to determine the best way to proceed and enhance the safety of our community. For more information go to: https://www.ithaca.edu/share.\nInformation shared in class assignments, class discussions, and at public events do not constitute an official disclosure, and faculty and staff do not have to report these to the Title IX Coordinator. Faculty and staff should be sure that access to campus and community resources related to sexual misconduct are available to students in the case these subjects do arise. Any other disclosure to faculty and staff can be reported to the Title IX Coordinator.\n\n\nAcademic Advising Center\nStudents are asked to consult with their faculty advisor, or the advising contact within their school, for all advising matters. Faculty advisors will be able to assist students with most advising questions, or they may collaborate with the dean’s office for more complicated matters. Students can find the name of their assigned faculty advisor in Homer or in Degree Works.\n\n\nDiversity and Inclusion\nIthaca College values diversity because it enriches our community and the myriad experiences that characterize an Ithaca College education. Diversity encompasses multiple dimensions, including but not limited to race, culture, nationality, ethnicity, religion, ideas, beliefs, geographic origin, class, sexual orientation, gender, gender identity and expression, disability, and age. We are dedicated to addressing current and past injustices and promoting excellence and equity. Ithaca College continually strives to build an inclusive and welcoming community of individuals with diverse talents and skills from a multitude of backgrounds who are committed to civility, mutual respect, social justice, and the free and open exchange of ideas. We commit ourselves to change, growth, and actions that embrace diversity as an integral part of the educational experience and of the community we create.Please learn more about Ithaca College’s commitment to diversity, equity and inclusion: https://www.ithaca.edu/diversity-and-inclusion/diversity-statement.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "If you have an iPhone, you can use the Notes app to scan a document to a PDF. To do this, follow the following steps:\n\nOpen the Notes app\nCreate a new note and name it.\nTap the camera icon and select “Scan Documents”. I suggest you use the manual capture option as this allows you to capture only once the camera is in focus.\nPosition your phone over the document and tap the shutter button. Make sure the camera is directly over the document and is in focus.\nContinue to the next page if there are multiple pages.\nTap “Save” when you are finished scanning all the pages.\n\nIf you have an Android device, you can use the Google Drive app to scan a document to a PDF. To do this, follow the following steps:\n\nOpen the Google Drive app\nTap the “+” icon in the bottom right corner\nTap “Scan”\nPosition your phone over the document and tap the shutter button. Make sure the camera is directly over the document and is in focus.\nContinue to the next page if there are multiple pages.\nTap the checkmark when you are finished scanning all the pages.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#how-do-i-use-my-mobile-device-to-scan-a-document-to-a-pdf",
    "href": "course-faq.html#how-do-i-use-my-mobile-device-to-scan-a-document-to-a-pdf",
    "title": "FAQ",
    "section": "",
    "text": "If you have an iPhone, you can use the Notes app to scan a document to a PDF. To do this, follow the following steps:\n\nOpen the Notes app\nCreate a new note and name it.\nTap the camera icon and select “Scan Documents”. I suggest you use the manual capture option as this allows you to capture only once the camera is in focus.\nPosition your phone over the document and tap the shutter button. Make sure the camera is directly over the document and is in focus.\nContinue to the next page if there are multiple pages.\nTap “Save” when you are finished scanning all the pages.\n\nIf you have an Android device, you can use the Google Drive app to scan a document to a PDF. To do this, follow the following steps:\n\nOpen the Google Drive app\nTap the “+” icon in the bottom right corner\nTap “Scan”\nPosition your phone over the document and tap the shutter button. Make sure the camera is directly over the document and is in focus.\nContinue to the next page if there are multiple pages.\nTap the checkmark when you are finished scanning all the pages.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 240: Basic Statistics with R",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule may change as the semester progresses and the timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\nWEEK\nDATE\nPREPARE\nTOPIC\nMATERIALS\nDUE\n\n\n\n\n2\nMon, Jan 27\n\nIntroductions\nGetting started with R \n\n\nCreate posit cloud account \nR and RStudio interface \n\n\n\n\nAssign Lab 1\n\n\n\n3\nMon, Feb 3\n\nBase R basics\n\n\nInstalling packages\nCreating vectors\nBasic data summarization - numerical\nBasic data summarization - visualizations\n\n\n\n\n\n\n\n4\nMon, Feb 10\n\n\n\n\n\n\n\n\n\n\n5\nMon, Feb 17\n\n\n\n\n\n\n\n\n\n\n6\nMon, Feb 24\n\n\n\n\n\n\n\n\n\n\n7\nMon, Mar 3\n\n\n\n\n\n\n\n\n\n\n8\nMon, Mar 10\n\n\n\n\n\n\n\n\n\n\n9\nMon, Mar 17\n\n\n\n\n\n\n\n\n\n\n10\nMon, Mar 24\n\n\n\n\n\n\n\n\n\n\n11\nMon, Mar 31\n\n\n\n\n\n\n\n\n\n\n12\nMon, Apr 7\n\n\n\n\n\n\n\n\n\n\n13\nMon, Apr 14\n\n\n\n\n\n\n\n\n\n\n14\nMon, Apr 21\n\n\n\n\n\n\n\n\n\n\n15\nMon, Apr 28\n\n\n\n\n\n\n\n\n\n\n16\nMon, May 5\n\n\n\n\n\n\n\n\n\n\n17\nMon, May 12",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course-downloads/datasets.html",
    "href": "course-downloads/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Currently unavailable!",
    "crumbs": [
      "Downloads",
      "Datasets"
    ]
  },
  {
    "objectID": "labs/Lab_4.html",
    "href": "labs/Lab_4.html",
    "title": "Data Visualization with ggplot2",
    "section": "",
    "text": "In today’s lab, we introduce a popular package called ggplot2 used for data visualization. This package is contained within the tidyverse package (already installed) so we do not need to install it separately. The term ggplot stands for “Grammar of Graphics Plot” which is a system for declaratively creating graphics. It provides a programmatic interface for specifying what variables to plot, how they are displayed, and general visual properties. The package allows users to create a wide variety of high-quality and customizable statistical graphs, making it a valuable tool for data exploration and presentation.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_4.html#introduction",
    "href": "labs/Lab_4.html#introduction",
    "title": "Data Visualization with ggplot2",
    "section": "",
    "text": "In today’s lab, we introduce a popular package called ggplot2 used for data visualization. This package is contained within the tidyverse package (already installed) so we do not need to install it separately. The term ggplot stands for “Grammar of Graphics Plot” which is a system for declaratively creating graphics. It provides a programmatic interface for specifying what variables to plot, how they are displayed, and general visual properties. The package allows users to create a wide variety of high-quality and customizable statistical graphs, making it a valuable tool for data exploration and presentation.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_4.html#creating-your-quarto-file",
    "href": "labs/Lab_4.html#creating-your-quarto-file",
    "title": "Data Visualization with ggplot2",
    "section": "Creating your Quarto File",
    "text": "Creating your Quarto File\nTo create your quarto file, follow the following steps:\n\nGo to File&gt;New File &gt; Quarto document. In the title field use Introducing ggplot2 then write your name under the Author field. Change the output option to pdf.\nNext, save the document as Lab_04. If you did it correctly, the file Lab_04.qmd should appear under the files section with a .qmd extension.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_4.html#packages",
    "href": "labs/Lab_4.html#packages",
    "title": "Data Visualization with ggplot2",
    "section": "Packages",
    "text": "Packages\nWe will need the following packages in today’s lab:\n\n\nopenintro: contains some of the data sets that we will use.\n\ntidyverse: contains the ggplot2 package.\n\nCreate a code chunk and load the following packages using the code below. Be sure to hit the green button to run the packages.\nlibrary(openintro)\nlibrary(tidyverse)",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_4.html#loading-the-data",
    "href": "labs/Lab_4.html#loading-the-data",
    "title": "Data Visualization with ggplot2",
    "section": "Loading the Data",
    "text": "Loading the Data\nWe will need the following data sets: duke_forest and sinusitis both contained in the openintro package. Load the data sets using the commands below:\n\nCodedata(duke_forest)\ndata(sinusitis)\n\n\nBefore you proceed, examine these data to understand their context. You may run the command ?duke_forest and ?sinusitis to learn more about the data sets.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_4.html#visualizing-numerical-data",
    "href": "labs/Lab_4.html#visualizing-numerical-data",
    "title": "Data Visualization with ggplot2",
    "section": "Visualizing Numerical Data",
    "text": "Visualizing Numerical Data\nOne Numerical Variable\nWhen we have a single numerical variable, we can use tools such as dot plots, histograms, and box plots to visualize the distribution of the variable.\nDot Plot\nThere are three basic steps in creating visualizations using the ggplot2() function. This function can take several arguments. The three basic steps are:\n\n\nStep 1: Add the data argument. Create a new code chunk and run the code below. What do you notice?\n\n\nCodeggplot(data = duke_forest)\n\n\n\n\n\n\n\n\n\nStep 2: Add a mapping argument. This step defines how variables should be mapped on the axes. Since we are dealing with one variable here (price), we will just use the \\(x-axis\\). See code below:\n\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x = price)\n       )\n\n\n\n\n\n\n\n\n\nStep 3: Define the visual properties. To define the visual properties, we use the geom_** (short form for geometry) function. In our case, we want to use dotplot and so we add the function geom_dotplot(). Notice that you must have the parentheses because this is a function that can take more arguments (you will learn more about this) to further define the nature of the dots added.\n\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x = price)\n       ) + \n       geom_dotplot()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\nWe can further customize the dots by adding arguments inside the geom_dotplot() function. For example, we can use a blue color for the dots and fill them in using orange color as shown below:\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x = price)\n       ) + \n       geom_dotplot(color = \"blue\", fill = \"orange\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\nWe can also change the size of dots by adding the argument dotsize. For example, we can set the size of the dots to 0.5 as shown below:\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x = price)\n       ) + \n       geom_dotplot(color = \"blue\", fill = \"orange\", dotsize = 0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\nWe can also change the appearance of the canvas by changing the theme. The most commonly used theme is the minimal theme. To add this theme, just add the theme_minimal() function. This will remove the gray background and gridlines. See below:\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x = price)\n       ) + \n       geom_dotplot(color = \"blue\", fill = \"orange\", dotsize = 0.5) +\n       theme_minimal()\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\nHistogram\nAnother tool we can use to visualize the distribution of a single numerical variable is a histogram. To create a histogram for the price variable, we follow the exact same steps as above but only change the geom_dotplot part to geom_histogram. See below:\n\nCodeggplot(data=duke_forest, \n       mapping = aes(x = price)\n       ) + \n       geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nWe can further customize the histogram by adding arguments inside the geom_histogram() function. For example, we can change the color of the bars to blue and fill them in using ornage color as shown below:\n\nCodeggplot(data=duke_forest, \n       mapping = aes(x = price)\n       ) + \n       geom_histogram(color = \"blue\", fill = \"orange\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nYou can also change the binwidth of the histogram by adding the argument binwidth. For example, we can set the binwidth to 100,000 as shown below. Notice that we do not have a comma in the number 100 000.\n\nCodeggplot(data=duke_forest, \n       mapping = aes(x = price)\n       ) + \n       geom_histogram(color = \"blue\", fill = \"orange\", binwidth = 100000)\n\n\n\n\n\n\n\nBoxplot\nWe can also create a boxplot in the same way but change the geometry to geom_boxplot. See below:\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x=price)\n       ) + \n       geom_boxplot()\n\n\n\n\n\n\n\nWe can further customize the boxplot by adding arguments inside the geom_boxplot() function. For example, we can change the color of the lines to blue and fill in the box using orange color as shown below:\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x=price)\n       ) + \n       geom_boxplot(color = \"blue\", fill = \"orange\")\n\n\n\n\n\n\n\nQuestion:\n\nBy examining the tools created above, how would you describe the distribution of the prices? What is the advantage of dot plot over the histogram? What about the box plot over the histogram?\nTwo Numerical Variables\nScatter Plots\nIn many cases, we want to assess relationships between two numerical variables. A scatter plot is the most commonly used visualization tool for this. To create a scatter plot, you just need to specify the variable on the \\(x-axis\\) and the one on the \\(y-axis\\). In addition, you will use geom_point as the geometry.\nSuppose we want to assess the association between price and area. Use the code below:\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x = area, y = price)\n       ) + \n       geom_point()\n\n\n\n\n\n\n\nQuestion - What pattern do you notice from the above scatter plot? Is this what you expected?\nYou can add some layer of complexity to the scatter plot to allow you compare three variables. For example, you may want to compare the relationship between price and area by cooling. To do this, you use the color argument as shown below:\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x=area, y = price, color = cooling)\n       ) + \n       geom_point()\n\n\n\n\n\n\n\nQuestion - What patterns do you notice above? What can you say about the relationship between price and area by cooling?",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_4.html#visualizing-categorical-data",
    "href": "labs/Lab_4.html#visualizing-categorical-data",
    "title": "Data Visualization with ggplot2",
    "section": "Visualizing Categorical Data",
    "text": "Visualizing Categorical Data\nSingle categorical Variable\nYou can visualize a single categorical variable using tools such as simple bar plots and pie charts. However, pie charts are not recommended because they are not easy to interpret, especially when the frequencies are close. We will focus on bar plots.\nSimple Bar Plots\nThe data set sinusitis contains two categorical variables (group and self_reported_improvement). Suppose we want to visualize the distribution of cases by self_reported_improvement. We follow the same structure as above but change the geom() part to geom_bar(). See code below:\n\nCodeggplot(data = sinusitis, \n       mapping = aes(x = self_reported_improvement)\n       ) + \n       geom_bar()\n\n\n\n\n\n\n\nYou may want to color the bars so that each category shows a different color. You can do this by using adding the argument fill= in the aes. This will automatically fill the bars using some colors. See below:\n\nCodeggplot(data = sinusitis, \n       mapping = aes(x = self_reported_improvement, fill = self_reported_improvement)\n       ) + \n       geom_bar()\n\n\n\n\n\n\n\nPie Chart\nA pie chart can be seen as a special type of bar plot. Let us create a pie chart to visualize the distribution of the self_reported_improvement variable in the sinusitis data set. Study the code below:\n\nCodeggplot(sinusitis %&gt;% count(self_reported_improvement), aes(x = \"\", y = n, fill = self_reported_improvement)) + \n  geom_bar(stat = \"identity\") + \n  coord_polar(theta = \"y\") + \n  labs(title = \"Distribution of Groups in Sinusitis Dataset\", fill = \"Group\") +\n  theme_void()\n\n\n\n\n\n\n\nTwo Categorical Variables\nWe can use stacked bar plots or side-by-side bar plots to visualize two categorical variables. For example, we may want to visualize the relationship between group and self_reported_improvement from the sinuisitis data set.\nStacked Bar Plots\nWhen you have multiple categorical variables, a stacked bar plot can be a nice way to visualize the relationship between them. For example, we can visualize the relationship between self_reported_improvement and group by simply using the second variable (in this case group) in the fill argument inside the aes(). See below:\n\nCodeggplot(data=sinusitis, \n       mapping = aes(x = self_reported_improvement, fill = group)\n       ) +\n       geom_bar()\n\n\n\n\n\n\n\nAs you can see, stacked bar plots are not easy to interpret. If the bars are placed side-by-side, it becomes easier to interpret.\nSide-by-side Bar plots\nTo create a side-by-side by plot, we simply add the argument position=dodge in the geom() function. See below:\n\nCodeggplot(data = sinusitis, \n       mapping = aes(x = self_reported_improvement, fill = group)\n       ) +\n       geom_bar(position = \"dodge\")",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_4.html#numerical-variable-by-category",
    "href": "labs/Lab_4.html#numerical-variable-by-category",
    "title": "Data Visualization with ggplot2",
    "section": "Numerical variable by Category",
    "text": "Numerical variable by Category\nIt is common to find bar plots that compare a numerical variable across levels of a categorical variable using a specified summary statistic such as the mean or the raw scores. For example, if you wanted to visualize the mean price across the type of cooling for the houses in the duke_forest data set, we can do this by declaring a statistic inside the geom_bar() as shown below:\n\nCodeggplot(duke_forest, aes(x = cooling, y = price/1000, fill=cooling)) +\n  geom_bar(stat = \"summary\", fun=\"mean\") +\n  labs(x = \"Type of Cooling\", \n       y = \"Average House Price in thousands\", \n       title = \"Bar Plot for Average price by Type of Cooling\")\n\n\n\n\n\n\n\nWe can also visualize the distribution of the price variable across the cooling without summarizing the price variable. To achieve this, we can use a side-by-side box plot. See below:\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x = cooling, y = price/1000, fill = cooling)\n       ) + \n       geom_boxplot(position = \"dodge\") +\n       labs(x = \"Type of Cooling\", \n            y = \"House Price in thousands\", \n            title = \"Box Plot for House Price by Type of Cooling\")\n\n\n\n\n\n\n\nWe can also create density plots to visualize the distribution of the price variable by cooling. To do this, we use the geom_density() function. Study the code below before running it. Change the value of the alpha argument to 0.9 to see what happens.\n\nCodeggplot(data = duke_forest, \n       mapping = aes(x = price/1000, fill = cooling)\n       ) + \n       geom_density(alpha = .5) +\n       labs(x = \"House Price in thousands\", \n            y = \"Density\", \n            title = \"Density Plot for House Price by Type of Cooling\")",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_4.html#exercises",
    "href": "labs/Lab_4.html#exercises",
    "title": "Data Visualization with ggplot2",
    "section": "Exercises",
    "text": "Exercises\nInstructions:\n\nCreate a new quarto document and save it as lab_04_Exe. As for the title, use Data Visualization with ggplot2 Exercises. Write your name under the Author field. Change the output option to pdf.\nDelete everything on the page except the YAML. Then, load the openintro and tidyverse packages. Make sure to use include=FALSE in the code chunk for packages.\nCopy and paste each question into the document and have a code chunk below the question if it needs code to answer.\nOnce you are done, render your document to PDF and submit the pdf on Canvas.\n\nQuestions:\n\n(2 pts) There is a data set called loan50 contained in the openintro package. Load the data set into your quarto work space. Run the query ?loan50 in the console to learn more about the data then give a brief description of the data set. List any two categorical variables and any two numerical variables in the data set.\n(4 pts) Create a simple bar plot to visualize the distribution of the variable loan_purpose. Based on your bar plot, what is the most frequent reason for which people in the sample took loans? What is the least frequent reason(s)? NOTE: You may want to flip the coordinates to have the bars along the y-axis instead of the x-axis. This will provide more space for all categories to appear.\n(4 pts) Create a dot plot to visualize the distribution of the variable loan_amount. Comment on the distribution.\n(4 pts) Suppose we want to answer the question, “do people with higher total_income tend to have higher loan_amount?. Create a scatter plot to visualize the relationship between loan_amount and total_income. Describe this relationship.\n(4 pts) Create a new scatter plot that colors the dots on the scatter plot in question 5 above using the categorical variable has_second_income (i.e., whether someone has a second income or not). Comment on the new scatter plot.\nCreate a contingency table to summarize the counts of the variables loan_status and home_ownership. Comment on this table. Please look up how to create a contingency table in R using the tidyverse workflow.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_2.html",
    "href": "labs/Lab_2.html",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "In today’s lab, we introduce an interface known as quarto that is helpful in creating reproducible reports. Reproducible reports seamlessly integrate code and narrative within the same document in order to allow others to easily replicate your results by running the embedded code.\nQuarto documents can be set up to produce (render) reports in various formats including pdf, html, MS Word, among others.\nNote: If you are using the desktop version of quarto, you will need to download quarto from https://quarto.org/docs/get-started/ before you proceed.\n\nIn today’s lab, we will need the openintro and tidyverse packages. You should have these packages already but in case you don’t, you will need to install them first. To install a package run the command below in the console. Replace the package_name with the name of the package you want to install.\ninstall.packages(\"package_name\")\nAn alternative way to install a package in RStudio is to use the install button in the packages tab.\n\nTo create your First Quarto file, follow the following steps:\nGo to File&gt;New File &gt; Quarto document. See below:\n\nAfter clicking Quarto document,a pop up window will appear with fields for the title and author. Enter the title of the document as Introducing Quarto and Data Wrangling because that is what we are doing today.\nWrite your name in the author field. The output format can stay as HTML. You can always change these options even after creating the document. The popup window looks as follows:\n\nClick create to create the document. Note that the document appears with the name Untitled. Click on file then navigate to save then change the name to Lab_02. Remember, we do not want to use a space for the document name. Click on save. Your document should appear under the files section with the name Lab_02.qmd (notice that you do not type the .qmd part, it automatically comes). With this document saved here, you can always return to it any time and continue working.\nWhen you look at the body of the document, you will note that the it has both plain text and some code chunks. Anything that you want interpreted as code goes into the code chunks. Otherwise, you can use the white space outside code chunks. Note, however, that you can still comment out stuff that you do not want to run as code inside code chunks using the hash tag.\nTo add your own code chunk at a given location in the document, place the cursor these, then click on code then go to add new chunk. Alternatively, use the keyboard shortcut cmd+opt+I on a MacOS and control+opt+I on Windows PCs.\nBy default, the code chunks are for R code. Quarto supports other languages such as python, and julia. To use other languages, verify that it is supported, then change the r to python (you may have to click on source to see the architecture of code chunks.\n\n\nWhen you are ready to generate your report, you click on the render button. Put this to the test by clicking on the render button. Study the report. If you want any changes, you make them in the source file (quarto), then click on render to see the results. It takes a while to become good with editing quarto documents, but most of that stuff is easily found online.\nWhen you render a quarto document, quarto outputs your report by running all code chunks sequentially from top to bottom. All operations in code chunks should be laid out in the right order. For example, if operation A must run before operation B, the code chunk in which operation A runs should come before the code chunk in which operation B runs.\nOperations such as installing packages that should be done only in the console. If any code chunk has code for package installation, quarto will try to install the package every time you render the document (imagine if you had to install TikTok on your phone evry time you needed to use it!!).\nOperations such as activating (waking up) packages, i.e., library(package name), can be included in the code chunks.\n\nThe tidyverse is a collection of open-source R packages that help with data manipulation, exploration, and visualization. In the first lab, our code was mostly base R (not Tidyverse). In this lab, you be introduced to the tidyverse as a framework for working with data structures in R. Although we will use Tidyverse for most of this lab and future labs, there are certain commands that are easier to do in base R. So, expect to see both Tidyverse and base R code from time to time.\n\n\nThe top portion of your quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data representation for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\nWe will use the first code chunk to load (activate) any packages that are needed for a specific quarto document. For example, to day, we will need the following packages: openintro and tidyverse. Copy and paste the following code in the first code chunk:\nlibrary(openintro)\nlibrary(tidyverse)\nRun the code chunk with the packages to ensure that they are all working. If any of them is missing, R will prompt you to install them. Go ahead and do so.\n\nThere are several ways to load data into a quarto document. Today, we will explore the first way - loading data stored in a package (openintro to be specific). Remember that the openintro package contains the data that comes with the openintro text. To view a complete list of the data frames in openintro, visit the site https://www.openintro.org/data/. Today, we will use a data frame called nycflights contained in the openintro package. To load this data into your work space, you will use the following command after running the packages.\nTo learn more about this data frame, run the following code in the nycflights:\ndata(nycflights)\n\nCodedata(\"nycflights\")\n\n\nWhen you run the above code, a new object should appear in the environment area. Click on penguins to view and study the data.\nTo learn more about this data frame, you can run the command ?nycflights in the console. It is very important that you do this.\n\nThe creators of the tidyverse noted that it is “an opionated collection of R packages designed for data science.” It’s a suite of packages designed with a consistent philosophy and aesthetic. This is nice because all of the packages are designed to work well together, providing a consistent framework to do many of the most common tasks in R, including, but not limited to…\n\ndata manipulation (dplyr)\nreshaping data (tidyr)\ndata visualization (ggplot)\nworking with strings (stringr)\n\nIt is important to note that tidyverse functions are designed to be like grammar for their tasks, so we’ll be using functions that are named as verbs to discuss the tidyverse. The idea is that you can string these grammatical elements together to form more complex statements, just like with language.\n\nIn lab_01, we performed the analyses using base R code. We saw that to compute the mean of a given variable in a data frame, you use data_frame_name$variable_name.\nFor example, to compute the mean of the variable hour in the nycflights data, you would write your code as follows:\nmean(nycflights$hour)\nIn the tidyverse, the code for the mean of the bill length would be\nnyflights %&gt;% \nsummarize(avg = mean(hour))\nThe symbol %&gt;% is called a pipe and is very common in tidyverse. It takes anything on its left and sends it (pipes it) to the function on the right. Here, we are taking the nycflights data frame and piping it into the summarize function (the function for summary statistics). Inside the function, we specify the variable and the statistic (in this case the mean). We have chosen to name the output as avg but this could be changed.\nYou can compute other summary statistics in a similar manner as above and if there is an NA one way to deal with would be to remove it. In some cases, one would replace NA with the average of the other values.\n\nData wrangling, broadly speaking, means getting your data into a useful form for visualizing and modelling it. Hadley Wickham, who has developed a lot of the tidyverse, conceptualizes the main steps involved in data wrangling as follows:\n\nImporting your data\nTidying your data\nTransforming your data\n\nData is considered tidy if\n\nEach variable has its own column\nEach observation/case has its own row\nEach value has its own cell.\n\nThe figure below illustrate the idea of tidy data:\n\nIf your data is not already in tidy format when you import it, you can use functions from the tidyR package, e.g. gather() and spread(), to “reshape” your data to get it into tidy format.\nToday we are going to look at the following important functions: select(), mutate(), filter(), arrange(), group_by(), and drop_na().\n\nThe select function is used to select specific variables (columns) from a larger data set.\nThe nycflights data frame has many variables. Suppose we are interested in the variables carrier, origin, arrival delay, and destination only. We achieve this as follows:\n\nCodenycflights_a &lt;- nycflights %&gt;% \nselect (carrier, arr_delay, dest, origin)\n\n\nNote:\n\nA new object named nycflights_a appears in the environment area when you run the above code.\nThe variable spelling in your code must match the names used in the data frame. This is a common mistake many students make.\n\nThe mutate function alters (mutates) an existing variable in some way and creates a new column(variable) with a name that you designate. Suppose you want to create a new variable called converts the arrival delay to hours (note that it is provided in minutes). You can run the following code:\n\nCodenycflights_b &lt;- nycflights %&gt;% \n  mutate (arr_delay_hr = arr_delay/60)\n\n\nNote:\n\nAfter running the above code, the new data frame will have one more variable (at the end). This is the new variable (column) you created. Check it out.\nMost mathematical operations can be done as above as long as it makes sense to do so. For example, you cannot perform the divide operation on a variable such as origin.\n\nJust like we can select certain variables (columns) from a data set, it is possible to filter a certain rows (cases/observations) in a data set.\nSuppose you have you interested in flights that originated from JFK only. You can achieve this by using the filter function as follows:\n\nCodenycflights_c &lt;- nycflights %&gt;% \nfilter (origin == \"JFK\")\n\n\n\nGroup_by is a special kind of filtering that is commonly used alongside a summarize() function. For example, suppose you want to compute the average arrival delay by airport. To do this, you first use the group_by function to group the cases by origin, then run the summarize function and call the mean. See code below:\n\nCodenycflights %&gt;% \n  group_by (origin) %&gt;%\n  summarize(avg_arr_delay=mean(arr_delay))\n\n# A tibble: 3 × 2\n  origin avg_arr_delay\n  &lt;chr&gt;          &lt;dbl&gt;\n1 EWR             9.33\n2 JFK             5.98\n3 LGA             5.71\n\n\nHere, you can see the delay times by origin airport.\n\nSome statistics may not be computed with if the variable of interest has missing values (these appear as na in R). In such cases, it might be necessary to drop missing values before moving forward with your analysis. To drop missing values from a given column (variable), you use the drop_na() function as shown below:\nname_of_your_choice &lt;- original_data %&gt;% \ndrop_na(variable_name)\nIf you want to drop_na from multiple variables, you simply list the variables in the drop_na function separated by commas.\n\nAnswer the following exercises. Be sure to show/explain your work on each question. You can copy and paste each question into the quarto document that you will submit. Have the code chunks below each question that requires code.\n\n(4 pts) Create a new quarto document with the title “Basic Data Wrangling”. Save the document as Lab_02_Exe. Delete everything on the page EXCEPT the YAML section. In the YAML area change the output format to pdf, and make sure your name is written under author.\n(4 pts) Create a code chunk and load the packages openintro and tidyverse. Run these packages to make sure they are properly loaded. Next, create a new code chunk and load the a data set called ames (this is contained in the openintro package). How many variables and how many observations/cases does this data fame have?\n(4 pts) Examine the detailed documentation of this dataset by clicking on the link https://jse.amstat.org/v19n3/decock/DataDocumentation.txt. What does the variable Bsmt.Qual measure? Be sure to provide information on each level/value of this variable.\n(2 pts) Create a new data set and save it as ames_new. This new data set should have the variables price, Bsmt.Qual, and Lot.Shape only.\n(2 pts) Determine the number of cases in the ames_new data frame that have the value Gd for the variable Bsmt.Qual. To do this, you may use the filter() function.\n(6 pts) Use the group_by() function to create a summary table that shows the average house prices by lot shape (Lot.Shape). What can you comment based on this result?\n\n\n\n\n\n\n\nSubmission Checklist\n\n\n\n\nAttempt all questions\nRender your work to pdf. Do not take pictures of your screen.\nSubmit the pdf to Canvas\nIf code errors prevent you from rendering your quarto file, you can suppress them (look this up). This should be a last resort thing. Consider seeking support early if you need it.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#introduction",
    "href": "labs/Lab_2.html#introduction",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "In today’s lab, we introduce an interface known as quarto that is helpful in creating reproducible reports. Reproducible reports seamlessly integrate code and narrative within the same document in order to allow others to easily replicate your results by running the embedded code.\nQuarto documents can be set up to produce (render) reports in various formats including pdf, html, MS Word, among others.\nNote: If you are using the desktop version of quarto, you will need to download quarto from https://quarto.org/docs/get-started/ before you proceed.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#installing-packages",
    "href": "labs/Lab_2.html#installing-packages",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "In today’s lab, we will need the openintro and tidyverse packages. You should have these packages already but in case you don’t, you will need to install them first. To install a package run the command below in the console. Replace the package_name with the name of the package you want to install.\ninstall.packages(\"package_name\")\nAn alternative way to install a package in RStudio is to use the install button in the packages tab.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#creating-a-quarto-file",
    "href": "labs/Lab_2.html#creating-a-quarto-file",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "To create your First Quarto file, follow the following steps:\nGo to File&gt;New File &gt; Quarto document. See below:\n\nAfter clicking Quarto document,a pop up window will appear with fields for the title and author. Enter the title of the document as Introducing Quarto and Data Wrangling because that is what we are doing today.\nWrite your name in the author field. The output format can stay as HTML. You can always change these options even after creating the document. The popup window looks as follows:\n\nClick create to create the document. Note that the document appears with the name Untitled. Click on file then navigate to save then change the name to Lab_02. Remember, we do not want to use a space for the document name. Click on save. Your document should appear under the files section with the name Lab_02.qmd (notice that you do not type the .qmd part, it automatically comes). With this document saved here, you can always return to it any time and continue working.\nWhen you look at the body of the document, you will note that the it has both plain text and some code chunks. Anything that you want interpreted as code goes into the code chunks. Otherwise, you can use the white space outside code chunks. Note, however, that you can still comment out stuff that you do not want to run as code inside code chunks using the hash tag.\nTo add your own code chunk at a given location in the document, place the cursor these, then click on code then go to add new chunk. Alternatively, use the keyboard shortcut cmd+opt+I on a MacOS and control+opt+I on Windows PCs.\nBy default, the code chunks are for R code. Quarto supports other languages such as python, and julia. To use other languages, verify that it is supported, then change the r to python (you may have to click on source to see the architecture of code chunks.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#notes-on-rendering",
    "href": "labs/Lab_2.html#notes-on-rendering",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "When you are ready to generate your report, you click on the render button. Put this to the test by clicking on the render button. Study the report. If you want any changes, you make them in the source file (quarto), then click on render to see the results. It takes a while to become good with editing quarto documents, but most of that stuff is easily found online.\nWhen you render a quarto document, quarto outputs your report by running all code chunks sequentially from top to bottom. All operations in code chunks should be laid out in the right order. For example, if operation A must run before operation B, the code chunk in which operation A runs should come before the code chunk in which operation B runs.\nOperations such as installing packages that should be done only in the console. If any code chunk has code for package installation, quarto will try to install the package every time you render the document (imagine if you had to install TikTok on your phone evry time you needed to use it!!).\nOperations such as activating (waking up) packages, i.e., library(package name), can be included in the code chunks.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#tidyverse-framework",
    "href": "labs/Lab_2.html#tidyverse-framework",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "The tidyverse is a collection of open-source R packages that help with data manipulation, exploration, and visualization. In the first lab, our code was mostly base R (not Tidyverse). In this lab, you be introduced to the tidyverse as a framework for working with data structures in R. Although we will use Tidyverse for most of this lab and future labs, there are certain commands that are easier to do in base R. So, expect to see both Tidyverse and base R code from time to time.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#the-structre-of-a-quarto-document",
    "href": "labs/Lab_2.html#the-structre-of-a-quarto-document",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "The top portion of your quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data representation for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\nWe will use the first code chunk to load (activate) any packages that are needed for a specific quarto document. For example, to day, we will need the following packages: openintro and tidyverse. Copy and paste the following code in the first code chunk:\nlibrary(openintro)\nlibrary(tidyverse)\nRun the code chunk with the packages to ensure that they are all working. If any of them is missing, R will prompt you to install them. Go ahead and do so.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#loading-data",
    "href": "labs/Lab_2.html#loading-data",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "There are several ways to load data into a quarto document. Today, we will explore the first way - loading data stored in a package (openintro to be specific). Remember that the openintro package contains the data that comes with the openintro text. To view a complete list of the data frames in openintro, visit the site https://www.openintro.org/data/. Today, we will use a data frame called nycflights contained in the openintro package. To load this data into your work space, you will use the following command after running the packages.\nTo learn more about this data frame, run the following code in the nycflights:\ndata(nycflights)\n\nCodedata(\"nycflights\")\n\n\nWhen you run the above code, a new object should appear in the environment area. Click on penguins to view and study the data.\nTo learn more about this data frame, you can run the command ?nycflights in the console. It is very important that you do this.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#a-little-more-about-tidyverse",
    "href": "labs/Lab_2.html#a-little-more-about-tidyverse",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "The creators of the tidyverse noted that it is “an opionated collection of R packages designed for data science.” It’s a suite of packages designed with a consistent philosophy and aesthetic. This is nice because all of the packages are designed to work well together, providing a consistent framework to do many of the most common tasks in R, including, but not limited to…\n\ndata manipulation (dplyr)\nreshaping data (tidyr)\ndata visualization (ggplot)\nworking with strings (stringr)\n\nIt is important to note that tidyverse functions are designed to be like grammar for their tasks, so we’ll be using functions that are named as verbs to discuss the tidyverse. The idea is that you can string these grammatical elements together to form more complex statements, just like with language.\n\nIn lab_01, we performed the analyses using base R code. We saw that to compute the mean of a given variable in a data frame, you use data_frame_name$variable_name.\nFor example, to compute the mean of the variable hour in the nycflights data, you would write your code as follows:\nmean(nycflights$hour)\nIn the tidyverse, the code for the mean of the bill length would be\nnyflights %&gt;% \nsummarize(avg = mean(hour))\nThe symbol %&gt;% is called a pipe and is very common in tidyverse. It takes anything on its left and sends it (pipes it) to the function on the right. Here, we are taking the nycflights data frame and piping it into the summarize function (the function for summary statistics). Inside the function, we specify the variable and the statistic (in this case the mean). We have chosen to name the output as avg but this could be changed.\nYou can compute other summary statistics in a similar manner as above and if there is an NA one way to deal with would be to remove it. In some cases, one would replace NA with the average of the other values.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#basic-data-wrangling",
    "href": "labs/Lab_2.html#basic-data-wrangling",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "Data wrangling, broadly speaking, means getting your data into a useful form for visualizing and modelling it. Hadley Wickham, who has developed a lot of the tidyverse, conceptualizes the main steps involved in data wrangling as follows:\n\nImporting your data\nTidying your data\nTransforming your data\n\nData is considered tidy if\n\nEach variable has its own column\nEach observation/case has its own row\nEach value has its own cell.\n\nThe figure below illustrate the idea of tidy data:\n\nIf your data is not already in tidy format when you import it, you can use functions from the tidyR package, e.g. gather() and spread(), to “reshape” your data to get it into tidy format.\nToday we are going to look at the following important functions: select(), mutate(), filter(), arrange(), group_by(), and drop_na().\n\nThe select function is used to select specific variables (columns) from a larger data set.\nThe nycflights data frame has many variables. Suppose we are interested in the variables carrier, origin, arrival delay, and destination only. We achieve this as follows:\n\nCodenycflights_a &lt;- nycflights %&gt;% \nselect (carrier, arr_delay, dest, origin)\n\n\nNote:\n\nA new object named nycflights_a appears in the environment area when you run the above code.\nThe variable spelling in your code must match the names used in the data frame. This is a common mistake many students make.\n\nThe mutate function alters (mutates) an existing variable in some way and creates a new column(variable) with a name that you designate. Suppose you want to create a new variable called converts the arrival delay to hours (note that it is provided in minutes). You can run the following code:\n\nCodenycflights_b &lt;- nycflights %&gt;% \n  mutate (arr_delay_hr = arr_delay/60)\n\n\nNote:\n\nAfter running the above code, the new data frame will have one more variable (at the end). This is the new variable (column) you created. Check it out.\nMost mathematical operations can be done as above as long as it makes sense to do so. For example, you cannot perform the divide operation on a variable such as origin.\n\nJust like we can select certain variables (columns) from a data set, it is possible to filter a certain rows (cases/observations) in a data set.\nSuppose you have you interested in flights that originated from JFK only. You can achieve this by using the filter function as follows:\n\nCodenycflights_c &lt;- nycflights %&gt;% \nfilter (origin == \"JFK\")\n\n\n\nGroup_by is a special kind of filtering that is commonly used alongside a summarize() function. For example, suppose you want to compute the average arrival delay by airport. To do this, you first use the group_by function to group the cases by origin, then run the summarize function and call the mean. See code below:\n\nCodenycflights %&gt;% \n  group_by (origin) %&gt;%\n  summarize(avg_arr_delay=mean(arr_delay))\n\n# A tibble: 3 × 2\n  origin avg_arr_delay\n  &lt;chr&gt;          &lt;dbl&gt;\n1 EWR             9.33\n2 JFK             5.98\n3 LGA             5.71\n\n\nHere, you can see the delay times by origin airport.\n\nSome statistics may not be computed with if the variable of interest has missing values (these appear as na in R). In such cases, it might be necessary to drop missing values before moving forward with your analysis. To drop missing values from a given column (variable), you use the drop_na() function as shown below:\nname_of_your_choice &lt;- original_data %&gt;% \ndrop_na(variable_name)\nIf you want to drop_na from multiple variables, you simply list the variables in the drop_na function separated by commas.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#exercises",
    "href": "labs/Lab_2.html#exercises",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "Answer the following exercises. Be sure to show/explain your work on each question. You can copy and paste each question into the quarto document that you will submit. Have the code chunks below each question that requires code.\n\n(4 pts) Create a new quarto document with the title “Basic Data Wrangling”. Save the document as Lab_02_Exe. Delete everything on the page EXCEPT the YAML section. In the YAML area change the output format to pdf, and make sure your name is written under author.\n(4 pts) Create a code chunk and load the packages openintro and tidyverse. Run these packages to make sure they are properly loaded. Next, create a new code chunk and load the a data set called ames (this is contained in the openintro package). How many variables and how many observations/cases does this data fame have?\n(4 pts) Examine the detailed documentation of this dataset by clicking on the link https://jse.amstat.org/v19n3/decock/DataDocumentation.txt. What does the variable Bsmt.Qual measure? Be sure to provide information on each level/value of this variable.\n(2 pts) Create a new data set and save it as ames_new. This new data set should have the variables price, Bsmt.Qual, and Lot.Shape only.\n(2 pts) Determine the number of cases in the ames_new data frame that have the value Gd for the variable Bsmt.Qual. To do this, you may use the filter() function.\n(6 pts) Use the group_by() function to create a summary table that shows the average house prices by lot shape (Lot.Shape). What can you comment based on this result?",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#submission",
    "href": "labs/Lab_2.html#submission",
    "title": "Reproducible Reports and Data Wrangling",
    "section": "",
    "text": "Submission Checklist\n\n\n\n\nAttempt all questions\nRender your work to pdf. Do not take pictures of your screen.\nSubmit the pdf to Canvas\nIf code errors prevent you from rendering your quarto file, you can suppress them (look this up). This should be a last resort thing. Consider seeking support early if you need it.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_1.html",
    "href": "labs/Lab_1.html",
    "title": "Hello, World!",
    "section": "",
    "text": "It is possible that some of you have heard/seen the phrase, Hello, World!, before. That is usually the first thing you learn in programming, i.e., to learn to write a computer program to print this sentence to screen. In this lab, we will not print Hello, World! to the screen. Instead, we will learn how to use R (via RStudio) to perform statistical analyses and to write reports in order to communicate the insights learned from data. While the analysis involves writing computer code, it is in no way going to involve writing computer programs. The goal of today’s lab is to introduce you to R and RStudio. You will learn to access and use RStudio, as well as perform the basic functions regarding statistical analysis. To clarify which is which: R is the name of the programming language itself and RStudio is a convenient interface (Integrated Development Environment or IDE) for working with R. I like to think about R as the car engine and RStudio as a nice driver dashboard. The engine drives the car, but the dashboard makes it easier for the driver to control the car. Most R users work with RStudio.\n\n\nCar Engine and Dashboard\n\nWe will work with the cloud (online) version of RStudio. To access RStudio online, click on the link https://posit.cloud to create an account or to sign in if you already have an account.\n\nAfter signing in to R studio, our next step is to create a new project. You can think of a project as a folder or simply a collection of files. Our project will be called “MATH 240 Spring 2025”. Each lab that you complete will be saved in this project. To create the project, you start by clicking on “New Project” and then change the default name (UNTITLED PROJECT) to “MATH 240 Spring 2025”.\nYay! You now have your project ready. In the next section, we explain the meaning of the various panels on your screen.\n\nYour new R studio project interface will look as follows:\n\n\n\nLeft Panel: The panel on the left is where the action happens. This panel is called the console. Every time you launch RStudio, it will have the same text at the top of the console telling you the version of R that you’re running. Below that information is the symbol ” &gt; “. This is where you enter your commands. When you enter and execute a command, the output will come right below it. These commands and their syntax have evolved over decades (literally) and now provide what many users feel is a fairly natural way to access data, organize, describe, and invoke statistical computations. Try typing 1 + 1 in the console and hit enter.\n\nCode1+1\n\n[1] 2\n\n\n\nUpper Right Panel: The panel in the upper right is called “environment”. It contains, among other things, the history of the actions or commands that you’ve previously entered.\nBottom Right Panel: The panel in the lower right contains tabs for browsing the files in your project folder, access help files for R functions, install and manage R packages, and inspecting visualizations through the viewer tab. By default, all data visualizations you make will appear directly below the code you used to create them. If you would rather your plots appear in the plots tab, you will need to change your global options.\n\nR is an open-source programming language, meaning that users can contribute packages that make our lives easier, and we can use them for free. Packages are simply pre-written code meant to serve specific purposes and may contain other packages inside them. Packages may also contain data sets. Packages are stored in a directory called “Library”. For this lab, and many others in the future, we will use the following two packages:\n\nThe tidyverse package is a very popular “umbrella” package which houses a suite of many different R packages: for data wrangling (including tidying) and data visualization.\nThe openintro package for data and custom functions with the OpenIntro resources. You will notice that the readings frequently refer to data contained in the OpenIntro Package. This is the package.\n\n\nThe command to install a package in R takes the following format:\ninstall.packages(\"package name\")\nTo install tidyverse and openintro, run the following commands:\ninstall.packages(\"tidyverse\")\ninstall.packages(\"openintro\")\nNote: You only need to install packages once, but you need to load them each time you relaunch RStudio. To call load (activate) the above installed packages, you use the following command:\nlibrary(tidyverse)\nlibrary(openintro)\nWhy Tidyverse? We are choosing to use the tidyverse package collection because it consists of a set of packages necessary for different aspects of working with data, anything from loading data to wrangling data to visualizing data to analyzing data. Additionally, these packages share common philosophies and are designed to work together. You can find more about the packages in the tidyverse at tidyverse.org.\n\nSuppose we want to find the mean of the numbers 23,24,26,19,18,25,21, and 39. The first thing you want to do is to get these data into R. We can achieve this by running the code below:\n\nCodex &lt;- c(23,24,26,19,18,25,21, 39) \n# We use the symbol &lt;- for assigning elements to an object. \n\n\nHere, we are creating a vector (a series of numbers) and storing it in an object called x. The symbols &lt;- is used for assignment. In R, we use a # to designate a comment (text that should not be evaluated as code). In above chunk, the text after # is a comment. Comments are a good way to document your code.\nTo find the mean of those numbers, we simply run the command mean(x) as shown below.\n\nCodemean(x)\n\n[1] 24.375\n\n\nThe general format is do_this(on_this). Here, do_this is the function while x is the thing on which we want an action taken.\nTo find the median of the numbers, we run the command median(x) as shown below.\n\nCodemedian(x)\n\n[1] 23.5\n\n\nWe can also create a string object (i.e., a series of non-numerical elements or characters). We use quotes for string characters. See below:\n\nCodey &lt;- c( \"Jane\", \"John\", \"Jess\", \"Jeff\", \"Joe\", \"Holli\", \"Henry\", \"Han\") \n# We use quotes for strings.\n\n\nNow, try to run the command mean(y):\n\nCodemean(y)\n\nWarning in mean.default(y): argument is not numeric or logical: returning NA\n\n\n[1] NA\n\n\nWhat do you get? Programming languages generally produce error messages when you try to perform an inappropriate operation or if there is a mistake in the code. Error messages are a good way to learn what you did wrong. In this case, the mean/average of the object y does not make sense because the entries of y are not numerical.\nYou can, however, perform other operations on y. For example, you may want to know how many Jane entries are in y. To do this, you may simply tabulate the entries in y as shown below:\n\nCodetable(y)\n\ny\n  Han Henry Holli  Jane  Jeff  Jess   Joe  John \n    1     1     1     1     1     1     1     1 \n\n\nWe see that there is only one Jane entry in y. While you could easily count the number of Jane entries in y, the table function is useful when you have many entries and you want to know how many times each entry appears.\nBefore we proceed, delete (or comment out) the mean(y) that you had written earlier.\n\nYou can create a data frame by combining vectors of equal length. Before we do that, let us create two more vectors (a numeric one and a character one).\n\nCodea&lt;-c(3,3,2,2,3,4,1,2)\n\nb&lt;-c(\"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\")\n\n\nYou can combine the vectors a, b, c, x, and y into a data frame (rectangular array) as follows. We store this in an object called practice_data and then print it.\n\nCodepractice_data &lt;- data.frame(name=y, age=x, sex=b, year=a)\nprint (practice_data)\n\n   name age sex year\n1  Jane  23   F    3\n2  John  24   M    3\n3  Jess  26   F    2\n4  Jeff  19   M    2\n5   Joe  18   M    3\n6 Holli  25   F    4\n7 Henry  21   M    1\n8   Han  39   M    2\n\n\nNote that for the object name, we use an underscore to separate the words practice and data. Do not use a blank space for object or variable names.\nNotice that the data frame looks like a more natural way that you are likely to encounter data. Most of the time, data is collected and stored in excel and can be imported into R for use. Throughout the course, you will learn how to import data from various sources into R.\n\nWe can run various statistics from data frames. Because the data frame combines many vectors (variables) we need to specify the data frame name and the variable we are targeting. For example, to find the mean of age, we write the data frame name and variable name separated by a dollar sign as follows mean(practice_data$age).\n\nCodemean(practice_data$age)\n\n[1] 24.375\n\n\nYou can also find other statistics. For median, the command is median(data$variable). For standard deviation, the command is sd(data$variable).\nYou can also run multiple summary statistics at once using the command summary(median(data$variable)). See below:\n\nCodesummary(practice_data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   20.50   23.50   24.38   25.25   39.00 \n\n\nThe summary command give you the minimum value, first quartile, median, mean, third quartile and maximum value.\n\nYou can create basic plots in base R:\n\nTo create a scatter plot to visualize the relationship between age and year, you can use the code below:\n\nCodeplot(practice_data$age, practice_data$year)\n\n\n\n\n\n\n\n\nTo create a bar plot to visualize the distribution of M and F, you can use the command below:\n\nCodebarplot(table((practice_data$sex)))\n\n\n\n\n\n\n\n\n\nCreate an object (name it income) containing the following numerical elements: 750, 810, 680, 1200, 1500, 1399,1525.\nFind the mean of the values in #1 above.\nCreate another object with names of your choosing and then combine it with the first object to make a data frame called my_data.\nRun the summary statistics for the income variable using my_data data frame that you created in #3 above.\nR comes with many pre-loaded data frames. One such data frame is called mtcars. Run the command ?mtcars to learn more about this data frame. Next, load this data frame into your work space by running the command data(mtcars).\nUse the mtcars data frame to find the median horsepower of the cars.\nCreate a histogram to visualize the distribution of the variable hp. What can you say about the distribution of hp based on the histogram?\n\n\n\n\n\n\n\nSubmission Checklist\n\n\n\n\nAttempt all questions\nCopy and past your work into a word document. Be sure to number your work properly.\nSubmit the word document to canvas.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#creating-a-new-project",
    "href": "labs/Lab_1.html#creating-a-new-project",
    "title": "Hello, World!",
    "section": "",
    "text": "After signing in to R studio, our next step is to create a new project. You can think of a project as a folder or simply a collection of files. Our project will be called “MATH 240 Spring 2025”. Each lab that you complete will be saved in this project. To create the project, you start by clicking on “New Project” and then change the default name (UNTITLED PROJECT) to “MATH 240 Spring 2025”.\nYay! You now have your project ready. In the next section, we explain the meaning of the various panels on your screen.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#the-r-studio-interface",
    "href": "labs/Lab_1.html#the-r-studio-interface",
    "title": "Hello, World!",
    "section": "",
    "text": "Your new R studio project interface will look as follows:\n\n\n\nLeft Panel: The panel on the left is where the action happens. This panel is called the console. Every time you launch RStudio, it will have the same text at the top of the console telling you the version of R that you’re running. Below that information is the symbol ” &gt; “. This is where you enter your commands. When you enter and execute a command, the output will come right below it. These commands and their syntax have evolved over decades (literally) and now provide what many users feel is a fairly natural way to access data, organize, describe, and invoke statistical computations. Try typing 1 + 1 in the console and hit enter.\n\nCode1+1\n\n[1] 2\n\n\n\nUpper Right Panel: The panel in the upper right is called “environment”. It contains, among other things, the history of the actions or commands that you’ve previously entered.\nBottom Right Panel: The panel in the lower right contains tabs for browsing the files in your project folder, access help files for R functions, install and manage R packages, and inspecting visualizations through the viewer tab. By default, all data visualizations you make will appear directly below the code you used to create them. If you would rather your plots appear in the plots tab, you will need to change your global options.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#r-packages",
    "href": "labs/Lab_1.html#r-packages",
    "title": "Hello, World!",
    "section": "",
    "text": "R is an open-source programming language, meaning that users can contribute packages that make our lives easier, and we can use them for free. Packages are simply pre-written code meant to serve specific purposes and may contain other packages inside them. Packages may also contain data sets. Packages are stored in a directory called “Library”. For this lab, and many others in the future, we will use the following two packages:\n\nThe tidyverse package is a very popular “umbrella” package which houses a suite of many different R packages: for data wrangling (including tidying) and data visualization.\nThe openintro package for data and custom functions with the OpenIntro resources. You will notice that the readings frequently refer to data contained in the OpenIntro Package. This is the package.\n\n\nThe command to install a package in R takes the following format:\ninstall.packages(\"package name\")\nTo install tidyverse and openintro, run the following commands:\ninstall.packages(\"tidyverse\")\ninstall.packages(\"openintro\")\nNote: You only need to install packages once, but you need to load them each time you relaunch RStudio. To call load (activate) the above installed packages, you use the following command:\nlibrary(tidyverse)\nlibrary(openintro)\nWhy Tidyverse? We are choosing to use the tidyverse package collection because it consists of a set of packages necessary for different aspects of working with data, anything from loading data to wrangling data to visualizing data to analyzing data. Additionally, these packages share common philosophies and are designed to work together. You can find more about the packages in the tidyverse at tidyverse.org.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#creating-vectors-in-r",
    "href": "labs/Lab_1.html#creating-vectors-in-r",
    "title": "Hello, World!",
    "section": "",
    "text": "Suppose we want to find the mean of the numbers 23,24,26,19,18,25,21, and 39. The first thing you want to do is to get these data into R. We can achieve this by running the code below:\n\nCodex &lt;- c(23,24,26,19,18,25,21, 39) \n# We use the symbol &lt;- for assigning elements to an object. \n\n\nHere, we are creating a vector (a series of numbers) and storing it in an object called x. The symbols &lt;- is used for assignment. In R, we use a # to designate a comment (text that should not be evaluated as code). In above chunk, the text after # is a comment. Comments are a good way to document your code.\nTo find the mean of those numbers, we simply run the command mean(x) as shown below.\n\nCodemean(x)\n\n[1] 24.375\n\n\nThe general format is do_this(on_this). Here, do_this is the function while x is the thing on which we want an action taken.\nTo find the median of the numbers, we run the command median(x) as shown below.\n\nCodemedian(x)\n\n[1] 23.5\n\n\nWe can also create a string object (i.e., a series of non-numerical elements or characters). We use quotes for string characters. See below:\n\nCodey &lt;- c( \"Jane\", \"John\", \"Jess\", \"Jeff\", \"Joe\", \"Holli\", \"Henry\", \"Han\") \n# We use quotes for strings.\n\n\nNow, try to run the command mean(y):\n\nCodemean(y)\n\nWarning in mean.default(y): argument is not numeric or logical: returning NA\n\n\n[1] NA\n\n\nWhat do you get? Programming languages generally produce error messages when you try to perform an inappropriate operation or if there is a mistake in the code. Error messages are a good way to learn what you did wrong. In this case, the mean/average of the object y does not make sense because the entries of y are not numerical.\nYou can, however, perform other operations on y. For example, you may want to know how many Jane entries are in y. To do this, you may simply tabulate the entries in y as shown below:\n\nCodetable(y)\n\ny\n  Han Henry Holli  Jane  Jeff  Jess   Joe  John \n    1     1     1     1     1     1     1     1 \n\n\nWe see that there is only one Jane entry in y. While you could easily count the number of Jane entries in y, the table function is useful when you have many entries and you want to know how many times each entry appears.\nBefore we proceed, delete (or comment out) the mean(y) that you had written earlier.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#creating-data-frames",
    "href": "labs/Lab_1.html#creating-data-frames",
    "title": "Hello, World!",
    "section": "",
    "text": "You can create a data frame by combining vectors of equal length. Before we do that, let us create two more vectors (a numeric one and a character one).\n\nCodea&lt;-c(3,3,2,2,3,4,1,2)\n\nb&lt;-c(\"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\")\n\n\nYou can combine the vectors a, b, c, x, and y into a data frame (rectangular array) as follows. We store this in an object called practice_data and then print it.\n\nCodepractice_data &lt;- data.frame(name=y, age=x, sex=b, year=a)\nprint (practice_data)\n\n   name age sex year\n1  Jane  23   F    3\n2  John  24   M    3\n3  Jess  26   F    2\n4  Jeff  19   M    2\n5   Joe  18   M    3\n6 Holli  25   F    4\n7 Henry  21   M    1\n8   Han  39   M    2\n\n\nNote that for the object name, we use an underscore to separate the words practice and data. Do not use a blank space for object or variable names.\nNotice that the data frame looks like a more natural way that you are likely to encounter data. Most of the time, data is collected and stored in excel and can be imported into R for use. Throughout the course, you will learn how to import data from various sources into R.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#using-data-frames",
    "href": "labs/Lab_1.html#using-data-frames",
    "title": "Hello, World!",
    "section": "",
    "text": "We can run various statistics from data frames. Because the data frame combines many vectors (variables) we need to specify the data frame name and the variable we are targeting. For example, to find the mean of age, we write the data frame name and variable name separated by a dollar sign as follows mean(practice_data$age).\n\nCodemean(practice_data$age)\n\n[1] 24.375\n\n\nYou can also find other statistics. For median, the command is median(data$variable). For standard deviation, the command is sd(data$variable).\nYou can also run multiple summary statistics at once using the command summary(median(data$variable)). See below:\n\nCodesummary(practice_data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   20.50   23.50   24.38   25.25   39.00 \n\n\nThe summary command give you the minimum value, first quartile, median, mean, third quartile and maximum value.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#basic-visualizations",
    "href": "labs/Lab_1.html#basic-visualizations",
    "title": "Hello, World!",
    "section": "",
    "text": "You can create basic plots in base R:\n\nTo create a scatter plot to visualize the relationship between age and year, you can use the code below:\n\nCodeplot(practice_data$age, practice_data$year)\n\n\n\n\n\n\n\n\nTo create a bar plot to visualize the distribution of M and F, you can use the command below:\n\nCodebarplot(table((practice_data$sex)))",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#exercises",
    "href": "labs/Lab_1.html#exercises",
    "title": "Hello, World!",
    "section": "",
    "text": "Create an object (name it income) containing the following numerical elements: 750, 810, 680, 1200, 1500, 1399,1525.\nFind the mean of the values in #1 above.\nCreate another object with names of your choosing and then combine it with the first object to make a data frame called my_data.\nRun the summary statistics for the income variable using my_data data frame that you created in #3 above.\nR comes with many pre-loaded data frames. One such data frame is called mtcars. Run the command ?mtcars to learn more about this data frame. Next, load this data frame into your work space by running the command data(mtcars).\nUse the mtcars data frame to find the median horsepower of the cars.\nCreate a histogram to visualize the distribution of the variable hp. What can you say about the distribution of hp based on the histogram?",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#submission",
    "href": "labs/Lab_1.html#submission",
    "title": "Hello, World!",
    "section": "",
    "text": "Submission Checklist\n\n\n\n\nAttempt all questions\nCopy and past your work into a word document. Be sure to number your work properly.\nSubmit the word document to canvas.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_3.html",
    "href": "labs/Lab_3.html",
    "title": "Importing Data into R",
    "section": "",
    "text": "The goal of this lab is to introduce you to the concept of importing data into R. In the previous labs, we have been using data that is already available in R or in R packages. However, in real life, you will often need to import data from external sources such as Excel, CSV, or text files. In this lab, we will learn how to “import” data into R from various sources including:\n\nSpreadsheets:\n\nExcel files\nGoogle Sheets\n\n\nCSV files\nText files\nDatabases",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#excel-files",
    "href": "labs/Lab_3.html#excel-files",
    "title": "Importing Data into R",
    "section": "Excel Files",
    "text": "Excel Files\nReading excel files into R is made possible by the readxl package (installed above).\nTo read an excel file into R, we use the read_excel() function from the readxl package. Follow these steps:\n\nObtain your excel data (.xslx) data set and upload it into your project (files section) using the upload option.\n\nClick on the file then choose the import dataset option (see figure below). This will open a pop-up window that shows a preview of the data set and the file path. Click on import.\n\n\nImporting an Excel Spreadsheet\n\n\nFinally, open a code chunk for your data and use the read_excel() function to read the file into R as shown below. If the file is in a folder, make sure to set the correct path.\n\nread_excel(\"filename.xlsx\")\nPractice: You received an email with an attachment of an excel data set named absenteeism. Read file into your R.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#google-sheets",
    "href": "labs/Lab_3.html#google-sheets",
    "title": "Importing Data into R",
    "section": "Google Sheets",
    "text": "Google Sheets\nGoogle is a popular cloud-based platform for storing and sharing data. To read data stored in Google sheets, we the package googlesheets4.\nThe fact that Google documents live in the cloud makes sharing and updating data easy especially if there are multiple people collaborating on the same project. Since googlesheets4 allows R to read the data from Google directly, you do not have to download the data. If you share a link with edit privileges, people can enter the data directly and you can analyze it in real-time.\nGoogle Sheets API\nSince R and Google Sheets are different products, the two have to communicate in some way. This happens through what is known as Application Programming Interface (API). APIs are means through which different applications are able to communicate with one another. Google has an API that allows R to communicate with Google Sheets. To use this API, you will need to authenticate your R session with Google every time.\nAuthenticating API with Google\nTo set up the connection between Google Sheets and R, start by running the command gs4_auth() in the console. This will open a new page in your browser that asks you which Google account you’d like to have access to. Click on the appropriate Google user to provide googlesheets4 access to the Google Sheets API. After this, you will be given a code that you will need to copy and paste into your R console. This will authenticate your R session with Google Sheets.\nReading Data from Google Sheets\nThe last step is to read the data into R. We use the function read_sheet() for this. The function takes the URL of the Google Sheet as an argument. The URL can be obtained by clicking on the share button on the Google Sheet and copying the link. Make sure you have the correct permissions to access the data.\nAs an example, we will read in a data set available on the link https://docs.google.com/spreadsheets/d/1pEJ6vuGmyWrNPjHSorvIANAAvGBctRgpp1_mEBZkN-Q/edit?usp=sharing This is the same absenteeism data set that we read from an excel file earlier. Use the code below (notice that we are saving the data as g_sheet):\ng_sheet &lt;-read_sheet(\"https://docs.google.com/spreadsheets/d/1pEJ6vuGmyWrNPjHSorvIANAAvGBctRgpp1_mEBZkN-Q/edit?gid=1024153496#gid=1024153496\")",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#relational-databases",
    "href": "labs/Lab_3.html#relational-databases",
    "title": "Importing Data into R",
    "section": "Relational Databases",
    "text": "Relational Databases\nIn a relational database, data is stored in tables, and the tables are related to each other. As an example, consider a college database that has several tables about their students including\n\n\nDemographics: ID, name, email, year of study, etc.\n\nCourses: ID, course name, credits, grade, department, etc.\n\nInstructors: ID, name, email, rank, etc.\n\nAlthough each table contains different information, there is a common column in each of the tables that allows information to be linked (in this case ID). The ID values are referred to as unique identifiers because they uniquely identify each student. No two students will have the same ID.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#advantages",
    "href": "labs/Lab_3.html#advantages",
    "title": "Importing Data into R",
    "section": "Advantages",
    "text": "Advantages\nStoring data in databases is considered secure, efficient, and reduces ambiguity.\n\nThe efficiency comes from the fact that databases are optimized for reading and writing data. Storing data in separate tables also limits the need to repeat information.\nThe security comes from the fact that databases can be set up to allow only authorized users to access certain data. If the data were all stored in one table, you lose this advantage.\nAmbiguity is reduced by, among other things, the fact that databases are better at handling multiple cases with same name but different characteristics.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#database-storage",
    "href": "labs/Lab_3.html#database-storage",
    "title": "Importing Data into R",
    "section": "Database Storage",
    "text": "Database Storage\nDatabases are stored in a database management system (DBMS). A DBMS is a software that allows you to interact with the database. The most common DBMS are MySQL and SQLite. In order to use data stored in a data base, there has to be a way to pull information from the database. Doing this is called querying the database. For example, one may want to know all female students who took MATH 240 at IC in the last 5 years. To do this, one would have to use SQL query commands. Using SQL commands is beyond the scope of this course, but you will learn how to use an R package known as RSQLite for establishing a connection to a database and using dbplr and dbplyr to work with relational data.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#connecting-to-a-database",
    "href": "labs/Lab_3.html#connecting-to-a-database",
    "title": "Importing Data into R",
    "section": "Connecting to a Database",
    "text": "Connecting to a Database\nWe are going to use a database called comany.db containing several tables with information about digital media (artistes, customers, artiste’s work, etc). We are going to focus only on artists and the albums tables. The unique identifier here will be artistId.\n\n\nImage credit: Carrie Wright, et al.\n\nFollow the following steps to connect to the database:\n\nDownload the database from this link and unzip it. Upload it to your project files.\n\nInstall and load the RSQLite package in case you haven’t already.\ninstall.packages(\"RSQLite\")\n# Do this in the console\nlibrary(RSQLite)\n# Do this in the packages code chunk\n\n\nSpecify driver and create a connection to the database.\n\nCodesqlite &lt;- dbDriver(\"SQLite\")\n\n\n\n\nCreate a connection to the database using the command below. Notice that the connection is saved as an object named con.\ncon &lt;- dbConnect(sqlite, \"company.db\")\n\n\nNote: that if you are using desktop version of R, you will need to specify a path to the database file.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#viewing-table-list",
    "href": "labs/Lab_3.html#viewing-table-list",
    "title": "Importing Data into R",
    "section": "Viewing Table List",
    "text": "Viewing Table List\nFirst, we want to know the tables contained in this database. To do this, use the function dbListTables() as shown below:\n\nCodedbListTables(con)\n\n [1] \"albums\"          \"artists\"         \"customers\"       \"employees\"      \n [5] \"genres\"          \"invoice_items\"   \"invoices\"        \"media_types\"    \n [9] \"playlist_track\"  \"playlists\"       \"sqlite_sequence\" \"sqlite_stat1\"   \n[13] \"tracks\"         \n\n\nNotice that there are 13 tables, but we are only interested in the artists and albums tables.\nAlthough here we are using a downloaded database, the common practice is to connect to databases remotely especially if they are huge in size.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#extracting-data",
    "href": "labs/Lab_3.html#extracting-data",
    "title": "Importing Data into R",
    "section": "Extracting Data",
    "text": "Extracting Data\nWe will use the dbplyr package to access the tables we are interested in. Start by loading the package in the packages chunk. The package works under the hood with dplyr to allow you to work with databases as if they were data frames. You will not notice that you are using the dbplyr.\nTo extract the artists table, use the command below:\n\nCodeartists &lt;- tbl(con, \"artists\")\n\n\nTo extract the albums table, use the command below:\n\nCodealbums &lt;- tbl(con, \"albums\")\n\n\nYou want to view the tables as tibbles (nice format that works well with tidyverse). To do this, use the as_tibble() function as shown below:\n\nCodeartists_tibble &lt;- as_tibble(artists)\n\n\n\nCodealbums_tibble &lt;- as_tibble(albums)",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#joining-tables-mutating-joins",
    "href": "labs/Lab_3.html#joining-tables-mutating-joins",
    "title": "Importing Data into R",
    "section": "Joining Tables: Mutating Joins",
    "text": "Joining Tables: Mutating Joins\nBefore you do your analyses, you may want to merge the tables. This is done using joins. There are several types of joins, but the most common ones are:\n\n\nInner join: This only keeps observations found in both left(x) and right (y) tables.\n\nLeft join: This keeps all observations in the left table (artists) and only those in the right table (albums) that match.\n\nRight join: This keeps all observations in the right table and only those in the left table that match.\n\nFull join: This keeps any observations in x or y.\n\nInner Joins\nBelow is a diagrammatic illustration of an inner join:\n\n\nImage credit: Carrie Wright, et al.\n\nTo join the artists and albums tables using an inner join, we use the function inner_join() as shown below. Notice that we are using tibble format for both albums and artists because tidyverse works best in this format.\n\nCodeinner_data &lt;- inner_join(artists_tibble, albums_tibble, by = \"ArtistId\")\n\n\nLeft Joins\nBelow is a diagrammatic illustration of a left join:\n\n\nImage credit: Carrie Wright, et al.\n\nTo join the artists and albums tables using a left join, we use the function left_join() as shown below:\n\nCodeleft_data &lt;- left_join(artists_tibble, albums_tibble, by = \"ArtistId\")\n\n\nRight Joins\nA right join is similar to a left join, but it keeps all observations in the right table and only those in the left table that match. Right joins return rows with NA values in the left table where there is no match. To join the artists and albums tables using a right join, we use the function right_join() as shown below.\n\nCoderight_data &lt;- right_join(artists_tibble, albums_tibble, by = \"ArtistId\")\n\n\nNote: The fact that 347 rows are present with the right join and 418 were present after the left join suggests that there are artists in the artists table without albums in the albums table.\nFull Joins\nLastly, a full join keeps any observations in x or y. NAs are used when data are missing for an observation(s). To join the artists and albums tables using a full join, we use the function full_join() as shown below.\n\nCodefull_data &lt;- full_join(artists_tibble, albums_tibble, by = \"ArtistId\")\n\n\nNote:\n\nTo learn more about joins, Garrick Aden-Buie has a great resource on GitHub. Check it out: here.\nOur focus was on mutating joins. There are also the aspect of filtering joins that focuses on observations (rows).",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_5.html",
    "href": "labs/Lab_5.html",
    "title": "Introducing Statistical Inference with Infer",
    "section": "",
    "text": "In today’s lab, we will start working on statistical inference using the infer package. The infer package aims to simplify and clarify the process of statistical inference by providing an expressive and consistent grammar. The package seamlessly integrates with tidyverse and shifts the focus from memorizing specific statistical tests to understanding the underlying principles of inference.\nInfer is centered around four main verbs namely:\n\n\nspecify(): This verb is used to specify the variables of interest and the structure of the data.\n\nhypothesize(): This verb is used to declare the null hypothesis.\n\ngenerate(): This verb is used to simulate the null distribution.\n\ncalculate(): This verb is used to calculate the statistic of interest for each replicate in the simulation.\n\nOptionally, you can use the visualize() verb to visualize the null distribution and the observed statistic.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/Lab_5.html#introduction",
    "href": "labs/Lab_5.html#introduction",
    "title": "Introducing Statistical Inference with Infer",
    "section": "",
    "text": "In today’s lab, we will start working on statistical inference using the infer package. The infer package aims to simplify and clarify the process of statistical inference by providing an expressive and consistent grammar. The package seamlessly integrates with tidyverse and shifts the focus from memorizing specific statistical tests to understanding the underlying principles of inference.\nInfer is centered around four main verbs namely:\n\n\nspecify(): This verb is used to specify the variables of interest and the structure of the data.\n\nhypothesize(): This verb is used to declare the null hypothesis.\n\ngenerate(): This verb is used to simulate the null distribution.\n\ncalculate(): This verb is used to calculate the statistic of interest for each replicate in the simulation.\n\nOptionally, you can use the visualize() verb to visualize the null distribution and the observed statistic.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/Lab_5.html#creating-your-quarto-file",
    "href": "labs/Lab_5.html#creating-your-quarto-file",
    "title": "Introducing Statistical Inference with Infer",
    "section": "Creating your Quarto File",
    "text": "Creating your Quarto File\nTo create your quarto file, follow the following steps:\n\nGo to File&gt;New File &gt; Quarto document. In the title field use Introducing Statistical Inference with Infer then write your name under the Author field. Change the output option to pdf.\nNext, save the document as Lab_05. If you did it correctly, the file Lab_05.qmd should appear under the files section with a .qmd extension.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/Lab_5.html#packages",
    "href": "labs/Lab_5.html#packages",
    "title": "Introducing Statistical Inference with Infer",
    "section": "Packages",
    "text": "Packages\nWe will need the following packages in today’s lab:\n\n\nopenintro: contains some of the data sets that we will use.\n\ntidyverse: contains the ggplot2 package.\n\ninfer: contains the functions we will use for statistical inference.\n\nInstall the infer package then create a code chunk and load the following packages. Be sure to hit the green button to run the packages.\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(infer)",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/Lab_5.html#loading-the-data",
    "href": "labs/Lab_5.html#loading-the-data",
    "title": "Introducing Statistical Inference with Infer",
    "section": "Loading the Data",
    "text": "Loading the Data\nWe will need the following data sets: duke_forest contained in the openintro package. Load the data sets using the commands below:\n\nCodedata(duke_forest)\n\n\nBefore you proceed, examine these data to understand their context. You may run the command ?duke_forest to learn more about the data sets.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/Lab_5.html#scenario-description-single-proportion",
    "href": "labs/Lab_5.html#scenario-description-single-proportion",
    "title": "Introducing Statistical Inference with Infer",
    "section": "Scenario Description (Single Proportion)",
    "text": "Scenario Description (Single Proportion)\nSuppose an article claims that 40 percent of homes in Duke Forest have central air conditioning. You are skeptical of this claim and want to use the duke_forest data to investigate this.\nObserved Statistic\nBefore we start, we need to find the observed statistic. The observed statistic is the proportion of homes in Duke Forest with central air conditioning based on the data we have. Notice that the variable is named as cooling in the data and it has two levels (central air, and other). We can find the proportion by counting the cooling variable then dividing the count by the total. See below:\n\nCodeduke_forest %&gt;%\n  count(cooling) %&gt;%\n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  cooling     n  prop\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt;\n1 other      53 0.541\n2 central    45 0.459\n\n\nHere, we see that the proportion of homes with central air conditioning is 0.459.\nQuestion: Based on the observed statistic, what can you say about the claim that the proportion of houses in Duke Forest that have central air cooling is 40%?\nHypotheses Testing\nTo test the claim via a hypothesis test, we have to state the hypotheses first:\n\nNull Hypothesis (\\(H_0\\)): The proportion of homes in Duke Forest with central air conditioning is 0.4. In symbols, the null hypothesis is \\(H_0: p = 0.4\\).\nAlternative Hypothesis (\\(H_A\\)): The proportion of homes in Duke Forest with central air conditioning is not 0.4. In symbols, the alternative hypothesis is \\(H_A: p \\neq 0.4\\).\nPerforming the Hypothesis Test\nBelow are the steps to perform the hypothesis test using infer:\n\nspecify: Specify the variables. Be sure to set success to central because that is the level of interest.\nhypothesize: The next step is to declare the null hypothesis. In this case, we are testing whether the proportion of homes with central air conditioning is 0.70.\ngenerate: Simulate the null distribution by running. You may think of this step as drawing cards (with replacement) from a deck that has 70% of the cards marked “central”.\ncalculate: For each replicate from the step above, we compute the proportion of cards that are marked “central”.\n\nvisualize (optional): Visualize the null distribution and the observed statistic. Also show the location of the observed statistic on the null distribution.\n\nCodeduke_forest %&gt;%\n  specify(response = cooling, success = \"central\") %&gt;%\n  hypothesize(null = \"point\", p = 0.4) %&gt;%\n  generate(reps = 1000, type = \"draw\") %&gt;%\n  calculate(stat = \"prop\") %&gt;%\n  visualize()\n\n\n\n\n\n\n\n\n\nWe call the above distribution a “null distribution” as it models the null hypothesis. Notice that it is centered around the null value (0.4). We want to answer the question, given this distribution, how likely is it to observe a proportion as extreme as 0.459? To do this we can calculate the percent of data beyond 0.459 (on both sides). This number is the p-value. Before we compute the number, let us visualize the p-value on the null distribution. See below\n\nCodeduke_forest %&gt;%\n  specify(response = cooling, success = \"central\") %&gt;%\n  hypothesize(null = \"point\", p = 0.4) %&gt;%\n  generate(reps = 1000, type = \"draw\") %&gt;%\n  calculate(stat = \"prop\") %&gt;%\n  visualize()+\n  shade_p_value(obs_stat = 0.459, direction = \"both\")\n\n\n\n\n\n\n\nThe vertical vertical line shows the location of the observed statistic (0.459) on the null distribution. The shaded regions represent the p-value. The p-value is the probability of observing a proportion as extreme as 0.459 or more extreme, assuming the null hypothesis is true.\nCalculating the p-value\nTo calculate the p-value, we can use the get_p_value() function. The argument obs_stat is used to specify the observed statistic while the direction argument is for specifying whether the test is left/right, or both-sided. In our case, it is both. Note also that we have dropped the visualize() function from the pipeline. See code below:\n\nCodeduke_forest %&gt;%\n  specify(response = cooling, success = \"central\") %&gt;%\n  hypothesize(null = \"point\", p = 0.4) %&gt;%\n  generate(reps = 1000, type = \"draw\") %&gt;%\n  calculate(stat = \"prop\") %&gt;%\n  get_p_value(obs_stat = 0.459, direction = \"both\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.288\n\n\nInterpreting the p-value\nThe p-value obtained is fairly large (over 20%). This means that the observed proportion of 0.459 is not unusual assuming that the true proportion of homes with central air conditioning is 0.4. Therefore, the data does not provide enough evidence to reject the claim that the proportion of homes with central heating is different from 0.4.\nConfidence Interval\nNotice that in the hypothesis test, our conclusion was that we do not have sufficient evidence to reject the null hypothesis. A natural question that may arise here is, “what is the plausible range of values within which we expect the true proportion to be?” This range of values is known as a confidence interval. We often set a confidence level for each interval (e.g., 95%, 99%). The confidence level represents the proportion of intervals that will contain the true parameter if we were to repeat the sampling process many times.\nTop find the confidence interval using infer, we make slight modifications to the pipeline:\n\nspecify: This stays the same as for the hypothesis test.\ngenerate: We generate the bootstrap distribution by resampling the data many times. Bootstrap resampling is a method that involves drawing samples with replacement from the original data set. Notice that because we are taking samples from the original sample, we expect the distribution to be centered around the observed statistic which arises from the data.\ncalculate: We calculate the confidence interval. The conf_int() function is used to calculate the confidence interval. The argument level is used to specify the confidence level. In our case, we will use a 95% confidence level.\n\nget_ci: Finally, we can get the lower and the upper limits of the confidence interval as follows:\n\nCodeduke_forest %&gt;%\n  specify(response = cooling, success = \"central\") %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"prop\", conf_level = 0.95) %&gt;%\n  get_ci()\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.357    0.561\n\n\n\nLet us save the above interval as CI. We will need this for visualizing the interval.\n\n\nCodeCI &lt;- duke_forest %&gt;%\n  specify(response = cooling, success = \"central\") %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"prop\", conf_level = 0.95) %&gt;%\n  get_ci()\n\n\n\nAn optional step is to visualize the confidence interval. This can be done using the visualize() function. See below:\n\n\nCodeduke_forest %&gt;%\n  specify(response = cooling, success = \"central\") %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"prop\", conf_level = 0.95) %&gt;%\n  visualize() +\n  shade_confidence_interval(endpoints = CI)\n\n\n\n\n\n\n\nInterpreting the Confidence Interval\nThe confidence interval obtained is (0.41, 0.51). This means that we are 95% confident that the true proportion of homes in Duke Forest with central air conditioning is between 0.41 and 0.51. This interval includes the null value of 0.4, which is consistent with our hypothesis test results obtained earlier. Note that these results are based on the data we have and may not be current.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/Lab_5.html#exercises",
    "href": "labs/Lab_5.html#exercises",
    "title": "Introducing Statistical Inference with Infer",
    "section": "Exercises",
    "text": "Exercises\nInstructions:\n\nCreate a new quarto document and save it as lab_05_Exe. Use the title Introduction to Infer. Write your name under the Author field. Change the output option to pdf.\nDelete everything on the page except the YAML. Then, load the packages openintro, infer, and tidyverse. Make sure to use include=FALSE in the code chunk for packages.\nCopy and paste each question into the document and have a code chunk below the question if it needs code to answer.\nOnce you are done, render your document to PDF and submit the pdf on Canvas.\n\nQuestions:\n\n(2 pts) There is a data set called age_at_mar contained in the openintro package. The data provides ages at which thousands of women in US got married. Import this data set. use the mutate function to create a new variable called mar_by_24? that shows whether a woman was married by age 24. The variable should have two levels: Yes and No.\n(4 pts) Create a simple bar plot to visualize the distribution of the variable mar_by_24?.\n(4 pts) What proportion of women were married by age 24?\n(4 pts) An article from 2010 claims that only 35% of women in US get married at or before age 24. The goal is to use the age_at_mar data to test this claim statistically via hypothesis testing. State the null and alternative hypotheses to be tested.\n(4 pts) Use the infer workflow to perform the hypothesis test from number 4 above. Be sure to interpret the results in context.\nUse the infer workflow to create a 95% confidence interval to estimate the proportion of women that got married by age 24. Interpret the interval in context and state whether it is consistent with the hypothesis test results.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "course-downloads/templates.html",
    "href": "course-downloads/templates.html",
    "title": "Templates",
    "section": "",
    "text": "Currently unavailable.",
    "crumbs": [
      "Downloads",
      "Templates"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "About the Course",
    "section": "",
    "text": "This course will be taught using a flipped model. A flipped classroom is one in which students engage with the course material before class. Class time is spent addressing any questions that students may have about the material, delving deeper into the material, and applying the material.\nConsistent with the above philosophy, majority of class time will be spent working on problems and/or activities individually or collaboratively, sharing your ideas/solutions, asking and investigating questions. Research shows that some of the deepest learning comes out of analyzing misconceptions and methods that don’t work, so we will strive to make failures productive. I may give mini–lectures to set the context for the class activities, but my main role will be to support and facilitate your deeper understanding of the course content. This course will place a lot of emphasis on real-life applications of statistics and will, from time to time, require you to collect data from the real world and use statistical concepts learned to draw useful insights. Here are some benefits associated with using Flipped Classroom Models:\n\nIndependent Learning Skills: not everything you learn in school will be applicable directly in your job or in the real world. In most cases, you need to transfer your knowledge to new contexts and that often involves new learning (often on your own). A flipped class model sets you up for success as an independent learner. You will learn how to learn on your own.\nActive Learning : In a flipped classroom, you will be actively engaged in the learning process. You will be asked to think, to write, to discuss, to solve problems, to analyze, to create, to evaluate, and to apply. This is a much more effective way to learn than passively listening to a lecture.\nCatching up : If you miss a given class and you had completed your CPA, that means you will still have some understanding of the basic ideas. You do not miss out entirely and that means catching up is easier."
  },
  {
    "objectID": "course-overview.html#corse-format-and-philosophy",
    "href": "course-overview.html#corse-format-and-philosophy",
    "title": "About the Course",
    "section": "",
    "text": "This course will be taught using a flipped model. A flipped classroom is one in which students engage with the course material before class. Class time is spent addressing any questions that students may have about the material, delving deeper into the material, and applying the material.\nConsistent with the above philosophy, majority of class time will be spent working on problems and/or activities individually or collaboratively, sharing your ideas/solutions, asking and investigating questions. Research shows that some of the deepest learning comes out of analyzing misconceptions and methods that don’t work, so we will strive to make failures productive. I may give mini–lectures to set the context for the class activities, but my main role will be to support and facilitate your deeper understanding of the course content. This course will place a lot of emphasis on real-life applications of statistics and will, from time to time, require you to collect data from the real world and use statistical concepts learned to draw useful insights. Here are some benefits associated with using Flipped Classroom Models:\n\nIndependent Learning Skills: not everything you learn in school will be applicable directly in your job or in the real world. In most cases, you need to transfer your knowledge to new contexts and that often involves new learning (often on your own). A flipped class model sets you up for success as an independent learner. You will learn how to learn on your own.\nActive Learning : In a flipped classroom, you will be actively engaged in the learning process. You will be asked to think, to write, to discuss, to solve problems, to analyze, to create, to evaluate, and to apply. This is a much more effective way to learn than passively listening to a lecture.\nCatching up : If you miss a given class and you had completed your CPA, that means you will still have some understanding of the basic ideas. You do not miss out entirely and that means catching up is easier."
  },
  {
    "objectID": "course-overview.html#course-objectives",
    "href": "course-overview.html#course-objectives",
    "title": "About the Course",
    "section": "Course Objectives",
    "text": "Course Objectives\nUpon successful completion of this course, students will be able to:\n\nDemonstrate an understanding of data and its types, and various data collection methods.\nChoose and use various basic statistical tools (numerical and visual) for summarizing data.\nDevelop basic proficiency in using statistical software (e.g., Tinker plots) for data analysis.\nDemonstrate a basic understanding of the statistical inference techniques including hypothesis testing and parameter estimation.\nUse simulation-based methods to perform statistical inference using real-world data set.\nApply statistical concepts learned to design and implement a study, and write a report to communicate insights to non-technical audiences."
  },
  {
    "objectID": "course-overview.html#meeting-times-location",
    "href": "course-overview.html#meeting-times-location",
    "title": "About the Course",
    "section": "Meeting Times & Location",
    "text": "Meeting Times & Location\n\n\n\nDay\nRoom/Hall\nTime\n\n\n\n\nMonday\nWilliams Hall 317\n11:00 - 11:50 am\n\n\nWednesday\nWilliams Hall 317\n11:00 - 11:50 am\n\n\nFriday\nWilliams Hall 317\n11:00 - 11:50 am"
  },
  {
    "objectID": "course-overview.html#prerequisites",
    "href": "course-overview.html#prerequisites",
    "title": "About the Course",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrerequisites include math placement in group 3 or higher, math placement assessment score of 46 or greater, or completion of MATH 10000 or MATH 18000 with a grade of C- or better. (F,S,Y)."
  },
  {
    "objectID": "course-overview.html#workload-expectations",
    "href": "course-overview.html#workload-expectations",
    "title": "About the Course",
    "section": "Workload Expectations",
    "text": "Workload Expectations\nThis is a 3-credit course. Credit is earned at Ithaca College in credit hours as measured by the Carnegie unit. The Carnegie unit is defined as one hour of classroom instruction and two hours of assignments outside the classroom, for a period of 15 weeks for each unit (credit)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course Support",
    "section": "",
    "text": "There are various resources available to help you succeed in this course and other college math courses. Should you feel like you are struggling too much, please don’t hesitate to reach out to me so we can discuss possible ways forward. Below are some support services available to you.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#open-hours",
    "href": "course-support.html#open-hours",
    "title": "Course Support",
    "section": "Open Hours",
    "text": "Open Hours\n\nI will be available during open hours (Mon 1.00 - 2.00 pm and Fri 10.00-11 am) to answer questions or concerns that you may have in the course. You can simply walk in during the stated times above. Even if you don’t have any question, you can just stop by to say hi. If these times don’t work for you, please click this link to see more options or email me directly. Open hours may be held in-person or virtually depending on your preference and the circumstances of the day. Below are the zoom link and password for virtual meetings:\nZoom Link: click here\nPassword: 850 424",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#mathematics-support-center",
    "href": "course-support.html#mathematics-support-center",
    "title": "Course Support",
    "section": "Mathematics Support Center",
    "text": "Mathematics Support Center\nThe mathematics department is committed to the success of all students enrolled in mathematics courses. Free one-on-one support for your mathematics coursework is available during select daytime and evening hours Monday-Friday at the Mathematics Room (Williams Hall 209). The Mathematics Room is staffed by mathematics faculty and vetted students. Student tutors offer support to fellow students in courses numbered 200 and below while math faculty offer support in any of the math courses. For more information and the schedule, please visit the Math Support Center.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#discussion-forums",
    "href": "course-support.html#discussion-forums",
    "title": "Course Support",
    "section": "Discussion Forums",
    "text": "Discussion Forums\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The Canvas Discussion forums, might be the best venue for these! There is a chance another student has already asked a similar question, so please check the other posts on before asking a new question. If you know the answer to a question that is posted, I encourage you to respond! In Canvas discussion section, anybody is able to start threads and others can jump in with replies.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course Support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go to Canvas Discussion forums), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). My email address is jgeteregechi@ithaca.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “MATH 155” in the subject line. Barring extenuating circumstances, I will respond to MATH 155 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday. If I take too long, please reach out again or talk to me before or after class as I may have missed your email.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#tutoring-and-academic-enrichment-services",
    "href": "course-support.html#tutoring-and-academic-enrichment-services",
    "title": "Course Support",
    "section": "Tutoring and Academic Enrichment Services",
    "text": "Tutoring and Academic Enrichment Services\nAs a supplement to faculty advising and office hours, Tutoring and Academic Enrichment Services offers exceptional peer resources free of charge. Learning Coaches provide content-specific peer tutoring in a variety of courses. Peer Success Coaches mentor students who wish to develop collegiate-level academic and social engagement skills. To access these courses and for more information, please visit the Center for Student Success.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#writing-center",
    "href": "course-support.html#writing-center",
    "title": "Course Support",
    "section": "Writing Center",
    "text": "Writing Center\nThe Writing Center aims to help students from all disciplines, backgrounds, and experiences to develop greater independence as writers. We are committed to helping students see writing as central to critical and creative thinking. The physical location in Smiddy 107 will not be open to clients. For more information and scheduling appointments please visit the writing center website.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course Support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Please visit the Mental Health Services for more information. Ithaca College encourages all students to access these resources.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-costs",
    "href": "course-support.html#course-costs",
    "title": "Course Support",
    "section": "Course costs",
    "text": "Course costs\n\nTextbooks: The textbooks for this course are open-source and freely available on the web.\nLaptops: Each student is expected to have a laptop they can bring to each lecture and lab.\nSoftware subscriptions:\n\nTinkerplots: We will use a software called Tinkerplots for this course. More information on how to get the software will be provided later. Expect to pay $10 fo this software.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-canvas",
    "href": "course-support.html#assistance-with-zoom-or-canvas",
    "title": "Course Support",
    "section": "Assistance with Zoom or Canvas",
    "text": "Assistance with Zoom or Canvas\nFor technical help with Canvas or Zoom, contact the Ithaca College IT support team at (607) 274-1000 or email them at servicedesk@ithaca.edu",
    "crumbs": [
      "Course information",
      "Support"
    ]
  }
]